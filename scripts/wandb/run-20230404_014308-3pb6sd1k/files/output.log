****Using Fake Training Data with Normal Distribution****
****Using Fake Training Data with Normal Distribution****
BetaVAE(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Linear(in_features=2304, out_features=2304, bias=True)
  (fc_var): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder_input): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (0): ConvTranspose2d(32, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv2d(32, 2, kernel_size=(3, 1), stride=(1, 1), padding=same)
    (4): Tanh()
  )
)
Epoch [1/30], Step [1/62], Loss: 291326720.0000
Epoch [1/30], Step [11/62], Loss: 154067040.0000
Epoch [1/30], Step [21/62], Loss: 39031384.0000
Epoch [1/30], Step [31/62], Loss: 16201860.0000
Epoch [1/30], Step [41/62], Loss: 8690743.0000
Epoch [1/30], Step [51/62], Loss: 17575404.0000
Epoch [1/30], Step [61/62], Loss: 31434980.0000
Epoch [2/30], Step [1/62], Loss: 20214238.0000
Epoch [2/30], Step [11/62], Loss: 7664746.0000
Epoch [2/30], Step [21/62], Loss: 13746379.0000
Epoch [2/30], Step [31/62], Loss: 6221127.0000
Epoch [2/30], Step [41/62], Loss: 15121092.0000
Epoch [2/30], Step [51/62], Loss: 2722146.7500
Epoch [2/30], Step [61/62], Loss: 20340154.0000
Epoch [3/30], Step [1/62], Loss: 19146266.0000
Epoch [3/30], Step [11/62], Loss: 2256344.0000
Epoch [3/30], Step [21/62], Loss: 2064483.8750
Epoch [3/30], Step [31/62], Loss: 5108042.0000
Epoch [3/30], Step [41/62], Loss: 2483394.5000
Epoch [3/30], Step [51/62], Loss: 3776731.0000
Epoch [3/30], Step [61/62], Loss: 1558319.5000
Epoch [4/30], Step [1/62], Loss: 2711173.7500
Epoch [4/30], Step [11/62], Loss: 690063.6250
Epoch [4/30], Step [21/62], Loss: 8629253.0000
Epoch [4/30], Step [31/62], Loss: 1334836.0000
Epoch [4/30], Step [41/62], Loss: 638472.1875
Epoch [4/30], Step [51/62], Loss: 96806.6641
Epoch [4/30], Step [61/62], Loss: 652639.6875
Epoch [5/30], Step [1/62], Loss: 107348.6953
Epoch [5/30], Step [11/62], Loss: 322545.7812
Epoch [5/30], Step [21/62], Loss: 7286201.0000
Epoch [5/30], Step [31/62], Loss: 64828.7656
Epoch [5/30], Step [41/62], Loss: 1150902.3750
Epoch [5/30], Step [51/62], Loss: 2036519.5000
Epoch [5/30], Step [61/62], Loss: 24354.8301
Epoch [6/30], Step [1/62], Loss: 7110986.0000
Epoch [6/30], Step [11/62], Loss: 244146.5938
Epoch [6/30], Step [21/62], Loss: 532884.5000
Epoch [6/30], Step [31/62], Loss: 3325989.7500
Epoch [6/30], Step [41/62], Loss: 1529479.5000
Epoch [6/30], Step [51/62], Loss: 2668211.2500
Epoch [6/30], Step [61/62], Loss: 2262781.7500
Epoch [7/30], Step [1/62], Loss: 330844.4688
Epoch [7/30], Step [11/62], Loss: 767674.1875
Epoch [7/30], Step [21/62], Loss: 363857.6250
Epoch [7/30], Step [31/62], Loss: 4680089.0000
Epoch [7/30], Step [41/62], Loss: 300878.0625
Epoch [7/30], Step [51/62], Loss: 796660.5625
Epoch [7/30], Step [61/62], Loss: 41543.2578
Epoch [8/30], Step [1/62], Loss: 4697025.0000
Epoch [8/30], Step [11/62], Loss: 263794.0625
Epoch [8/30], Step [21/62], Loss: 2386264.2500
Epoch [8/30], Step [31/62], Loss: 327405.9375
Epoch [8/30], Step [41/62], Loss: 203072.7969
Epoch [8/30], Step [51/62], Loss: 535368.7500
Epoch [8/30], Step [61/62], Loss: 7300126.0000
Epoch [9/30], Step [1/62], Loss: 775171.1875
Epoch [9/30], Step [11/62], Loss: 480146.1562
Epoch [9/30], Step [21/62], Loss: 325109.2500
Epoch [9/30], Step [31/62], Loss: 441341.7500
Epoch [9/30], Step [41/62], Loss: 2597999.7500
Epoch [9/30], Step [51/62], Loss: 481794.1875
Epoch [9/30], Step [61/62], Loss: 2862764.2500
Epoch [10/30], Step [1/62], Loss: 566414.6250
Epoch [10/30], Step [11/62], Loss: 18814.5781
Epoch [10/30], Step [21/62], Loss: 644294.9375
Epoch [10/30], Step [31/62], Loss: 431836.2500
Epoch [10/30], Step [41/62], Loss: 114484.3281
Epoch [10/30], Step [51/62], Loss: 2217741.0000
Epoch [10/30], Step [61/62], Loss: 41663.3008
Epoch [11/30], Step [1/62], Loss: 50470.2500
Epoch [11/30], Step [11/62], Loss: 338301.5312
Epoch [11/30], Step [21/62], Loss: 248604.6719
Epoch [11/30], Step [31/62], Loss: 303767.8125
Epoch [11/30], Step [41/62], Loss: 1736544.8750
Epoch [11/30], Step [51/62], Loss: 646869.8750
Epoch [11/30], Step [61/62], Loss: 457184.6875
Epoch [12/30], Step [1/62], Loss: 1161511.8750
Epoch [12/30], Step [11/62], Loss: 677050.5625
Epoch [12/30], Step [21/62], Loss: 220560.5625
Epoch [12/30], Step [31/62], Loss: 416390.3125
Epoch [12/30], Step [41/62], Loss: 462031.2500
Epoch [12/30], Step [51/62], Loss: 803197.0000
Epoch [12/30], Step [61/62], Loss: 248288.2969
Epoch [13/30], Step [1/62], Loss: 3953.1128
Epoch [13/30], Step [11/62], Loss: 625261.8125
Epoch [13/30], Step [21/62], Loss: 1784736.1250
Epoch [13/30], Step [31/62], Loss: 470459.6875
Epoch [13/30], Step [41/62], Loss: 191431.4219
Epoch [13/30], Step [51/62], Loss: 1696668.8750
Epoch [13/30], Step [61/62], Loss: 1149036.3750
Epoch [14/30], Step [1/62], Loss: 751859.8750
Epoch [14/30], Step [11/62], Loss: 247937.9375
Epoch [14/30], Step [21/62], Loss: 668094.1875
Epoch [14/30], Step [31/62], Loss: 2209763.2500
Epoch [14/30], Step [41/62], Loss: 744720.9375
Epoch [14/30], Step [51/62], Loss: 509845.0312
Epoch [14/30], Step [61/62], Loss: 481986.0312
Epoch [15/30], Step [1/62], Loss: 101994.5312
Epoch [15/30], Step [11/62], Loss: 221795.5781
Epoch [15/30], Step [21/62], Loss: 23565.7832
Epoch [15/30], Step [31/62], Loss: 163089.2969
Epoch [15/30], Step [41/62], Loss: 166884.9219
Epoch [15/30], Step [51/62], Loss: 629750.7500
Epoch [15/30], Step [61/62], Loss: 1193474.7500
Epoch [16/30], Step [1/62], Loss: 829328.5625
Epoch [16/30], Step [11/62], Loss: 1375578.3750
Epoch [16/30], Step [21/62], Loss: 1148370.1250
Epoch [16/30], Step [31/62], Loss: 205275.7969
Epoch [16/30], Step [41/62], Loss: 306288.7188
Epoch [16/30], Step [51/62], Loss: 83758.3672
Epoch [16/30], Step [61/62], Loss: 895434.8750
Epoch [17/30], Step [1/62], Loss: 611600.3750
Epoch [17/30], Step [11/62], Loss: 110471.2656
Epoch [17/30], Step [21/62], Loss: 546508.8125
Epoch [17/30], Step [31/62], Loss: 243210.5625
Epoch [17/30], Step [41/62], Loss: 167919.6562
Epoch [17/30], Step [51/62], Loss: 1669043.2500
Epoch [17/30], Step [61/62], Loss: 1652707.1250
Epoch [18/30], Step [1/62], Loss: 1196377.7500
Epoch [18/30], Step [11/62], Loss: 122249.1406
Epoch [18/30], Step [21/62], Loss: 1146291.5000
Epoch [18/30], Step [31/62], Loss: 564515.3750
Epoch [18/30], Step [41/62], Loss: 514661.0625
Epoch [18/30], Step [51/62], Loss: 353492.7500
Epoch [18/30], Step [61/62], Loss: 1027691.1250
Epoch [19/30], Step [1/62], Loss: 513909.3438
Epoch [19/30], Step [11/62], Loss: 398891.2188
Epoch [19/30], Step [21/62], Loss: 637680.3125
Epoch [19/30], Step [31/62], Loss: 236263.7656
Epoch [19/30], Step [41/62], Loss: 88552.4922
Epoch [19/30], Step [51/62], Loss: 824938.5625
Epoch [19/30], Step [61/62], Loss: 254381.4375
Epoch [20/30], Step [1/62], Loss: 1326529.0000
Epoch [20/30], Step [11/62], Loss: 780418.1875
Epoch [20/30], Step [21/62], Loss: 1187995.3750
Epoch [20/30], Step [31/62], Loss: 417318.3438
Epoch [20/30], Step [41/62], Loss: 1035697.4375
Epoch [20/30], Step [51/62], Loss: 39080.6328
Epoch [20/30], Step [61/62], Loss: 892324.9375
Epoch [21/30], Step [1/62], Loss: 739073.7500
Epoch [21/30], Step [11/62], Loss: 135032.6719
Epoch [21/30], Step [21/62], Loss: 1874512.8750
Epoch [21/30], Step [31/62], Loss: 1051247.8750
Epoch [21/30], Step [41/62], Loss: 21304.1445
Epoch [21/30], Step [51/62], Loss: 221585.5312
Epoch [21/30], Step [61/62], Loss: 1282344.1250
Epoch [22/30], Step [1/62], Loss: 42496.4531
Epoch [22/30], Step [11/62], Loss: 941265.8125
Epoch [22/30], Step [21/62], Loss: 1222855.6250
Epoch [22/30], Step [31/62], Loss: 2163495.5000
Epoch [22/30], Step [41/62], Loss: 673887.2500
Epoch [22/30], Step [51/62], Loss: 244404.5625
Epoch [22/30], Step [61/62], Loss: 388437.5000
Epoch [23/30], Step [1/62], Loss: 178711.6719
Epoch [23/30], Step [11/62], Loss: 170628.7969
Epoch [23/30], Step [21/62], Loss: 777488.9375
Epoch [23/30], Step [31/62], Loss: 705806.7500
Epoch [23/30], Step [41/62], Loss: 1273480.3750
Epoch [23/30], Step [51/62], Loss: 1050331.8750
Epoch [23/30], Step [61/62], Loss: 110040.6797
Epoch [24/30], Step [1/62], Loss: 616856.8125
Epoch [24/30], Step [11/62], Loss: 227996.8438
Epoch [24/30], Step [21/62], Loss: 231841.1094
Epoch [24/30], Step [31/62], Loss: 1357106.0000
Epoch [24/30], Step [41/62], Loss: 1027121.5625
Epoch [24/30], Step [51/62], Loss: 257033.3594
Epoch [24/30], Step [61/62], Loss: 360880.6250
Epoch [25/30], Step [1/62], Loss: 1251852.5000
Epoch [25/30], Step [11/62], Loss: 653363.2500
Epoch [25/30], Step [21/62], Loss: 253548.8750
Epoch [25/30], Step [31/62], Loss: 357402.5625
Epoch [25/30], Step [41/62], Loss: 561823.1250
Epoch [25/30], Step [51/62], Loss: 178198.3594
Epoch [25/30], Step [61/62], Loss: 190357.7031
Epoch [26/30], Step [1/62], Loss: 536185.0000
Epoch [26/30], Step [11/62], Loss: 266994.7188
Epoch [26/30], Step [21/62], Loss: 750743.8750
Epoch [26/30], Step [31/62], Loss: 1341276.6250
Epoch [26/30], Step [41/62], Loss: 1388327.1250
Epoch [26/30], Step [51/62], Loss: 590441.2500
Epoch [26/30], Step [61/62], Loss: 169819.3750
Epoch [27/30], Step [1/62], Loss: 1128197.0000
Epoch [27/30], Step [11/62], Loss: 910298.3750
Epoch [27/30], Step [21/62], Loss: 1152617.8750
Epoch [27/30], Step [31/62], Loss: 63725.4844
Epoch [27/30], Step [41/62], Loss: 429894.9375
Epoch [27/30], Step [51/62], Loss: 175123.7031
Epoch [27/30], Step [61/62], Loss: 121314.0625
Epoch [28/30], Step [1/62], Loss: 343005.6562
Epoch [28/30], Step [11/62], Loss: 1853375.0000
Epoch [28/30], Step [21/62], Loss: 903301.2500
Epoch [28/30], Step [31/62], Loss: 1789758.2500
Epoch [28/30], Step [41/62], Loss: 76351.6562
Epoch [28/30], Step [51/62], Loss: 346737.3750
Epoch [28/30], Step [61/62], Loss: 1298582.1250
Epoch [29/30], Step [1/62], Loss: 315857.8750
Epoch [29/30], Step [11/62], Loss: 660766.6250
Epoch [29/30], Step [21/62], Loss: 1217890.5000
Epoch [29/30], Step [31/62], Loss: 40324.6992
Epoch [29/30], Step [41/62], Loss: 1162504.3750
Epoch [29/30], Step [51/62], Loss: 11442.1982
Epoch [29/30], Step [61/62], Loss: 659423.8125
Epoch [30/30], Step [1/62], Loss: 492066.3750
Epoch [30/30], Step [11/62], Loss: 1141922.5000
Epoch [30/30], Step [21/62], Loss: 552139.7500
Epoch [30/30], Step [31/62], Loss: 283444.8750
Epoch [30/30], Step [41/62], Loss: 745313.3750
Epoch [30/30], Step [51/62], Loss: 976457.3750
Epoch [30/30], Step [61/62], Loss: 416178.7188
denselstm(
  (lstm): LSTM(256, 256, batch_first=True)
  (fc): Linear(in_features=6912, out_features=2304, bias=True)
)
Epoch [1/30], Step [1/62], Loss: 0.7961
Epoch [1/30], Step [11/62], Loss: 0.8024
Epoch [1/30], Step [21/62], Loss: 0.8039
Epoch [1/30], Step [31/62], Loss: 0.7971
Epoch [1/30], Step [41/62], Loss: 0.8005
Epoch [1/30], Step [51/62], Loss: 0.7998
Epoch [1/30], Step [61/62], Loss: 0.8036
Epoch [2/30], Step [1/62], Loss: 0.6917
Epoch [2/30], Step [11/62], Loss: 0.6824
Epoch [2/30], Step [21/62], Loss: 0.6767
Epoch [2/30], Step [31/62], Loss: 0.6835
Epoch [2/30], Step [41/62], Loss: 0.6893
Epoch [2/30], Step [51/62], Loss: 0.6923
Epoch [2/30], Step [61/62], Loss: 0.6986
Epoch [3/30], Step [1/62], Loss: 0.5299
Epoch [3/30], Step [11/62], Loss: 0.5280
Epoch [3/30], Step [21/62], Loss: 0.5505
Epoch [3/30], Step [31/62], Loss: 0.5563
Epoch [3/30], Step [41/62], Loss: 0.5641
Epoch [3/30], Step [51/62], Loss: 0.5789
Epoch [3/30], Step [61/62], Loss: 0.5854
Epoch [4/30], Step [1/62], Loss: 0.4194
Epoch [4/30], Step [11/62], Loss: 0.4257
Epoch [4/30], Step [21/62], Loss: 0.4357
Epoch [4/30], Step [31/62], Loss: 0.4501
Epoch [4/30], Step [41/62], Loss: 0.4524
Epoch [4/30], Step [51/62], Loss: 0.4720
Epoch [4/30], Step [61/62], Loss: 0.4750
Epoch [5/30], Step [1/62], Loss: 0.3514
Epoch [5/30], Step [11/62], Loss: 0.3510
Epoch [5/30], Step [21/62], Loss: 0.3625
Epoch [5/30], Step [31/62], Loss: 0.3693
Epoch [5/30], Step [41/62], Loss: 0.3769
Epoch [5/30], Step [51/62], Loss: 0.3829
Epoch [5/30], Step [61/62], Loss: 0.3895
Epoch [6/30], Step [1/62], Loss: 0.3040
Epoch [6/30], Step [11/62], Loss: 0.3099
Epoch [6/30], Step [21/62], Loss: 0.3140
Epoch [6/30], Step [31/62], Loss: 0.3176
Epoch [6/30], Step [41/62], Loss: 0.3185
Epoch [6/30], Step [51/62], Loss: 0.3238
Epoch [6/30], Step [61/62], Loss: 0.3326
Epoch [7/30], Step [1/62], Loss: 0.2605
Epoch [7/30], Step [11/62], Loss: 0.2650
Epoch [7/30], Step [21/62], Loss: 0.2697
Epoch [7/30], Step [31/62], Loss: 0.2773
Epoch [7/30], Step [41/62], Loss: 0.2809
Epoch [7/30], Step [51/62], Loss: 0.2791
Epoch [7/30], Step [61/62], Loss: 0.2850
Epoch [8/30], Step [1/62], Loss: 0.2389
Epoch [8/30], Step [11/62], Loss: 0.2366
Epoch [8/30], Step [21/62], Loss: 0.2379
Epoch [8/30], Step [31/62], Loss: 0.2432
Epoch [8/30], Step [41/62], Loss: 0.2464
Epoch [8/30], Step [51/62], Loss: 0.2494
Epoch [8/30], Step [61/62], Loss: 0.2489
Epoch [9/30], Step [1/62], Loss: 0.2140
Epoch [9/30], Step [11/62], Loss: 0.2179
Epoch [9/30], Step [21/62], Loss: 0.2191
Epoch [9/30], Step [31/62], Loss: 0.2171
Epoch [9/30], Step [41/62], Loss: 0.2221
Epoch [9/30], Step [51/62], Loss: 0.2232
Epoch [9/30], Step [61/62], Loss: 0.2248
Epoch [10/30], Step [1/62], Loss: 0.1996
Epoch [10/30], Step [11/62], Loss: 0.1992
Epoch [10/30], Step [21/62], Loss: 0.1990
Epoch [10/30], Step [31/62], Loss: 0.2037
Epoch [10/30], Step [41/62], Loss: 0.2012
Epoch [10/30], Step [51/62], Loss: 0.2025
Epoch [10/30], Step [61/62], Loss: 0.2030
Epoch [11/30], Step [1/62], Loss: 0.1873
Epoch [11/30], Step [11/62], Loss: 0.1870
Epoch [11/30], Step [21/62], Loss: 0.1849
Epoch [11/30], Step [31/62], Loss: 0.1866
Epoch [11/30], Step [41/62], Loss: 0.1873
Epoch [11/30], Step [51/62], Loss: 0.1872
Epoch [11/30], Step [61/62], Loss: 0.1874
Epoch [12/30], Step [1/62], Loss: 0.1754
Epoch [12/30], Step [11/62], Loss: 0.1769
Epoch [12/30], Step [21/62], Loss: 0.1752
Epoch [12/30], Step [31/62], Loss: 0.1731
Epoch [12/30], Step [41/62], Loss: 0.1739
Epoch [12/30], Step [51/62], Loss: 0.1744
Epoch [12/30], Step [61/62], Loss: 0.1749
Epoch [13/30], Step [1/62], Loss: 0.1681
Epoch [13/30], Step [11/62], Loss: 0.1669
Epoch [13/30], Step [21/62], Loss: 0.1665
Epoch [13/30], Step [31/62], Loss: 0.1646
Epoch [13/30], Step [41/62], Loss: 0.1657
Epoch [13/30], Step [51/62], Loss: 0.1638
Epoch [13/30], Step [61/62], Loss: 0.1636
Epoch [14/30], Step [1/62], Loss: 0.1556
Epoch [14/30], Step [11/62], Loss: 0.1591
Epoch [14/30], Step [21/62], Loss: 0.1587
Epoch [14/30], Step [31/62], Loss: 0.1581
Epoch [14/30], Step [41/62], Loss: 0.1562
Epoch [14/30], Step [51/62], Loss: 0.1573
Epoch [14/30], Step [61/62], Loss: 0.1568
Epoch [15/30], Step [1/62], Loss: 0.1528
Epoch [15/30], Step [11/62], Loss: 0.1543
Epoch [15/30], Step [21/62], Loss: 0.1533
Epoch [15/30], Step [31/62], Loss: 0.1510
Epoch [15/30], Step [41/62], Loss: 0.1506
Epoch [15/30], Step [51/62], Loss: 0.1510
Epoch [15/30], Step [61/62], Loss: 0.1500
Epoch [16/30], Step [1/62], Loss: 0.1465
Epoch [16/30], Step [11/62], Loss: 0.1472
Epoch [16/30], Step [21/62], Loss: 0.1459
Epoch [16/30], Step [31/62], Loss: 0.1463
Epoch [16/30], Step [41/62], Loss: 0.1455
Epoch [16/30], Step [51/62], Loss: 0.1458
Epoch [16/30], Step [61/62], Loss: 0.1445
Epoch [17/30], Step [1/62], Loss: 0.1405
Epoch [17/30], Step [11/62], Loss: 0.1434
Epoch [17/30], Step [21/62], Loss: 0.1413
Epoch [17/30], Step [31/62], Loss: 0.1415
Epoch [17/30], Step [41/62], Loss: 0.1408
Epoch [17/30], Step [51/62], Loss: 0.1390
Epoch [17/30], Step [61/62], Loss: 0.1398
Epoch [18/30], Step [1/62], Loss: 0.1378
Epoch [18/30], Step [11/62], Loss: 0.1394
Epoch [18/30], Step [21/62], Loss: 0.1380
Epoch [18/30], Step [31/62], Loss: 0.1360
Epoch [18/30], Step [41/62], Loss: 0.1347
Epoch [18/30], Step [51/62], Loss: 0.1352
Epoch [18/30], Step [61/62], Loss: 0.1355
Epoch [19/30], Step [1/62], Loss: 0.1381
Epoch [19/30], Step [11/62], Loss: 0.1352
Epoch [19/30], Step [21/62], Loss: 0.1369
Epoch [19/30], Step [31/62], Loss: 0.1336
Epoch [19/30], Step [41/62], Loss: 0.1322
Epoch [19/30], Step [51/62], Loss: 0.1321
Epoch [19/30], Step [61/62], Loss: 0.1312
Epoch [20/30], Step [1/62], Loss: 0.1334
Epoch [20/30], Step [11/62], Loss: 0.1316
Epoch [20/30], Step [21/62], Loss: 0.1304
Epoch [20/30], Step [31/62], Loss: 0.1307
Epoch [20/30], Step [41/62], Loss: 0.1305
Epoch [20/30], Step [51/62], Loss: 0.1285
Epoch [20/30], Step [61/62], Loss: 0.1288
Epoch [21/30], Step [1/62], Loss: 0.1252
Epoch [21/30], Step [11/62], Loss: 0.1282
Epoch [21/30], Step [21/62], Loss: 0.1293
Epoch [21/30], Step [31/62], Loss: 0.1274
Epoch [21/30], Step [41/62], Loss: 0.1261
Epoch [21/30], Step [51/62], Loss: 0.1256
Epoch [21/30], Step [61/62], Loss: 0.1254
Epoch [22/30], Step [1/62], Loss: 0.1218
Epoch [22/30], Step [11/62], Loss: 0.1256
Epoch [22/30], Step [21/62], Loss: 0.1266
Epoch [22/30], Step [31/62], Loss: 0.1240
Epoch [22/30], Step [41/62], Loss: 0.1235
Epoch [22/30], Step [51/62], Loss: 0.1239
Epoch [22/30], Step [61/62], Loss: 0.1219
Epoch [23/30], Step [1/62], Loss: 0.1244
Epoch [23/30], Step [11/62], Loss: 0.1243
Epoch [23/30], Step [21/62], Loss: 0.1228
Epoch [23/30], Step [31/62], Loss: 0.1219
Epoch [23/30], Step [41/62], Loss: 0.1219
Epoch [23/30], Step [51/62], Loss: 0.1203
Epoch [23/30], Step [61/62], Loss: 0.1206
Epoch [24/30], Step [1/62], Loss: 0.1200
Epoch [24/30], Step [11/62], Loss: 0.1211
Epoch [24/30], Step [21/62], Loss: 0.1204
Epoch [24/30], Step [31/62], Loss: 0.1197
Epoch [24/30], Step [41/62], Loss: 0.1196
Epoch [24/30], Step [51/62], Loss: 0.1180
Epoch [24/30], Step [61/62], Loss: 0.1179
Epoch [25/30], Step [1/62], Loss: 0.1185
Epoch [25/30], Step [11/62], Loss: 0.1187
Epoch [25/30], Step [21/62], Loss: 0.1184
Epoch [25/30], Step [31/62], Loss: 0.1184
Epoch [25/30], Step [41/62], Loss: 0.1174
Epoch [25/30], Step [51/62], Loss: 0.1167
Epoch [25/30], Step [61/62], Loss: 0.1165
Epoch [26/30], Step [1/62], Loss: 0.1175
Epoch [26/30], Step [11/62], Loss: 0.1174
Epoch [26/30], Step [21/62], Loss: 0.1159
Epoch [26/30], Step [31/62], Loss: 0.1159
Epoch [26/30], Step [41/62], Loss: 0.1163
Epoch [26/30], Step [51/62], Loss: 0.1149
Epoch [26/30], Step [61/62], Loss: 0.1148
Epoch [27/30], Step [1/62], Loss: 0.1158
Epoch [27/30], Step [11/62], Loss: 0.1161
Epoch [27/30], Step [21/62], Loss: 0.1145
Epoch [27/30], Step [31/62], Loss: 0.1141
Epoch [27/30], Step [41/62], Loss: 0.1138
Epoch [27/30], Step [51/62], Loss: 0.1132
Epoch [27/30], Step [61/62], Loss: 0.1124
Epoch [28/30], Step [1/62], Loss: 0.1140
Epoch [28/30], Step [11/62], Loss: 0.1133
Epoch [28/30], Step [21/62], Loss: 0.1131
Epoch [28/30], Step [31/62], Loss: 0.1124
Epoch [28/30], Step [41/62], Loss: 0.1124
Epoch [28/30], Step [51/62], Loss: 0.1124
Epoch [28/30], Step [61/62], Loss: 0.1106
Epoch [29/30], Step [1/62], Loss: 0.1129
Epoch [29/30], Step [11/62], Loss: 0.1125
Epoch [29/30], Step [21/62], Loss: 0.1120
Epoch [29/30], Step [31/62], Loss: 0.1109
Epoch [29/30], Step [41/62], Loss: 0.1099
Epoch [29/30], Step [51/62], Loss: 0.1101
Epoch [29/30], Step [61/62], Loss: 0.1104
Epoch [30/30], Step [1/62], Loss: 0.1126
Epoch [30/30], Step [11/62], Loss: 0.1126
Epoch [30/30], Step [21/62], Loss: 0.1088
Epoch [30/30], Step [31/62], Loss: 0.1095
Epoch [30/30], Step [41/62], Loss: 0.1083
Epoch [30/30], Step [51/62], Loss: 0.1083
Epoch [30/30], Step [61/62], Loss: 0.1072
Traceback (most recent call last):
  File "run_vae_lstm.py", line 84, in <module>
    plt_compare_series([gt_test[:3], reconst[:3]], 'Predictions', label_list=['Ground Truth', 'Predictions'])
  File "/home/jupyter-chuanhao/data/chuanhao_anomaly/vaelstm/utils/myplot.py", line 13, in plt_compare_series
    fig, ax = plt.figure()
TypeError: cannot unpack non-iterable Figure object
