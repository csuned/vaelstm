****Using Fake Training Data with Normal Distribution****
****Using Fake Training Data with Normal Distribution****
BetaVAE(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Linear(in_features=2304, out_features=2304, bias=True)
  (fc_var): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder_input): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (0): ConvTranspose2d(32, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv2d(32, 2, kernel_size=(3, 1), stride=(1, 1), padding=same)
    (4): Tanh()
  )
)
Epoch [1/30], Step [1/62], Loss: 290146944.0000
Epoch [1/30], Step [11/62], Loss: 130419312.0000
Epoch [1/30], Step [21/62], Loss: 27647646.0000
Epoch [1/30], Step [31/62], Loss: 12030082.0000
Epoch [1/30], Step [41/62], Loss: 27488716.0000
Epoch [1/30], Step [51/62], Loss: 5933256.0000
Epoch [1/30], Step [61/62], Loss: 42181384.0000
Epoch [2/30], Step [1/62], Loss: 8514878.0000
Epoch [2/30], Step [11/62], Loss: 15929442.0000
Epoch [2/30], Step [21/62], Loss: 3499829.5000
Epoch [2/30], Step [31/62], Loss: 6359646.5000
Epoch [2/30], Step [41/62], Loss: 3216405.2500
Epoch [2/30], Step [51/62], Loss: 2467871.0000
Epoch [2/30], Step [61/62], Loss: 10738721.0000
Epoch [3/30], Step [1/62], Loss: 7565832.5000
Epoch [3/30], Step [11/62], Loss: 3056129.5000
Epoch [3/30], Step [21/62], Loss: 1795265.3750
Epoch [3/30], Step [31/62], Loss: 2645583.5000
Epoch [3/30], Step [41/62], Loss: 12827811.0000
Epoch [3/30], Step [51/62], Loss: 10016397.0000
Epoch [3/30], Step [61/62], Loss: 1013280.0625
Epoch [4/30], Step [1/62], Loss: 1396322.0000
Epoch [4/30], Step [11/62], Loss: 1676662.7500
Epoch [4/30], Step [21/62], Loss: 436752.2500
Epoch [4/30], Step [31/62], Loss: 284147.1562
Epoch [4/30], Step [41/62], Loss: 262120.3750
Epoch [4/30], Step [51/62], Loss: 10584620.0000
Epoch [4/30], Step [61/62], Loss: 367099.0000
Epoch [5/30], Step [1/62], Loss: 8466.1387
Epoch [5/30], Step [11/62], Loss: 2278025.7500
Epoch [5/30], Step [21/62], Loss: 84833.2812
Epoch [5/30], Step [31/62], Loss: 110873.9531
Epoch [5/30], Step [41/62], Loss: 857985.3750
Epoch [5/30], Step [51/62], Loss: 9787236.0000
Epoch [5/30], Step [61/62], Loss: 9643054.0000
Epoch [6/30], Step [1/62], Loss: 2198751.7500
Epoch [6/30], Step [11/62], Loss: 1389012.8750
Epoch [6/30], Step [21/62], Loss: 820279.2500
Epoch [6/30], Step [31/62], Loss: 3150303.2500
Epoch [6/30], Step [41/62], Loss: 157160.9531
Epoch [6/30], Step [51/62], Loss: 364594.4062
Epoch [6/30], Step [61/62], Loss: 157163.5781
Epoch [7/30], Step [1/62], Loss: 1394807.2500
Epoch [7/30], Step [11/62], Loss: 777427.4375
Epoch [7/30], Step [21/62], Loss: 326910.7812
Epoch [7/30], Step [31/62], Loss: 531101.5625
Epoch [7/30], Step [41/62], Loss: 325446.9688
Epoch [7/30], Step [51/62], Loss: 2099839.7500
Epoch [7/30], Step [61/62], Loss: 2018599.0000
Epoch [8/30], Step [1/62], Loss: 7491314.5000
Epoch [8/30], Step [11/62], Loss: 1198681.3750
Epoch [8/30], Step [21/62], Loss: 563582.3125
Epoch [8/30], Step [31/62], Loss: 417346.9062
Epoch [8/30], Step [41/62], Loss: 440316.7812
Epoch [8/30], Step [51/62], Loss: 7260759.0000
Epoch [8/30], Step [61/62], Loss: 258071.6719
Epoch [9/30], Step [1/62], Loss: 1389968.2500
Epoch [9/30], Step [11/62], Loss: 638071.8125
Epoch [9/30], Step [21/62], Loss: 663777.2500
Epoch [9/30], Step [31/62], Loss: 321421.5000
Epoch [9/30], Step [41/62], Loss: 3142104.0000
Epoch [9/30], Step [51/62], Loss: 47719.9727
Epoch [9/30], Step [61/62], Loss: 133473.8906
Epoch [10/30], Step [1/62], Loss: 330954.4375
Epoch [10/30], Step [11/62], Loss: 270040.8750
Epoch [10/30], Step [21/62], Loss: 219455.2500
Epoch [10/30], Step [31/62], Loss: 4520688.0000
Epoch [10/30], Step [41/62], Loss: 133637.0938
Epoch [10/30], Step [51/62], Loss: 3623685.2500
Epoch [10/30], Step [61/62], Loss: 1050787.6250
Epoch [11/30], Step [1/62], Loss: 6833824.0000
Epoch [11/30], Step [11/62], Loss: 931789.3750
Epoch [11/30], Step [21/62], Loss: 61489.5938
Epoch [11/30], Step [31/62], Loss: 272861.1250
Epoch [11/30], Step [41/62], Loss: 278448.4688
Epoch [11/30], Step [51/62], Loss: 690717.9375
Epoch [11/30], Step [61/62], Loss: 90698.1406
Epoch [12/30], Step [1/62], Loss: 295703.4062
Epoch [12/30], Step [11/62], Loss: 1033861.7500
Epoch [12/30], Step [21/62], Loss: 2645788.5000
Epoch [12/30], Step [31/62], Loss: 263062.1250
Epoch [12/30], Step [41/62], Loss: 561940.8125
Epoch [12/30], Step [51/62], Loss: 695730.4375
Epoch [12/30], Step [61/62], Loss: 735289.8125
Epoch [13/30], Step [1/62], Loss: 492212.5312
Epoch [13/30], Step [11/62], Loss: 496569.5000
Epoch [13/30], Step [21/62], Loss: 2055716.0000
Epoch [13/30], Step [31/62], Loss: 737777.9375
Epoch [13/30], Step [41/62], Loss: 718084.6875
Epoch [13/30], Step [51/62], Loss: 679501.8750
Epoch [13/30], Step [61/62], Loss: 1083394.2500
Epoch [14/30], Step [1/62], Loss: 1744937.6250
Epoch [14/30], Step [11/62], Loss: 451403.1562
Epoch [14/30], Step [21/62], Loss: 618543.8750
Epoch [14/30], Step [31/62], Loss: 508851.1562
Epoch [14/30], Step [41/62], Loss: 798184.8750
Epoch [14/30], Step [51/62], Loss: 794188.2500
Epoch [14/30], Step [61/62], Loss: 1191705.5000
Epoch [15/30], Step [1/62], Loss: 19963.0391
Epoch [15/30], Step [11/62], Loss: 3115882.2500
Epoch [15/30], Step [21/62], Loss: 91209.4297
Epoch [15/30], Step [31/62], Loss: 803395.2500
Epoch [15/30], Step [41/62], Loss: 1093921.8750
Epoch [15/30], Step [51/62], Loss: 623581.1875
Epoch [15/30], Step [61/62], Loss: 1274743.1250
Epoch [16/30], Step [1/62], Loss: 276820.6562
Epoch [16/30], Step [11/62], Loss: 527381.4375
Epoch [16/30], Step [21/62], Loss: 40758.3867
Epoch [16/30], Step [31/62], Loss: 243910.0938
Epoch [16/30], Step [41/62], Loss: 440787.5625
Epoch [16/30], Step [51/62], Loss: 1649005.3750
Epoch [16/30], Step [61/62], Loss: 463737.9688
Epoch [17/30], Step [1/62], Loss: 165503.7500
Epoch [17/30], Step [11/62], Loss: 632843.1250
Epoch [17/30], Step [21/62], Loss: 369944.0938
Epoch [17/30], Step [31/62], Loss: 1432433.6250
Epoch [17/30], Step [41/62], Loss: 424510.4688
Epoch [17/30], Step [51/62], Loss: 117656.9609
Epoch [17/30], Step [61/62], Loss: 881201.5000
Epoch [18/30], Step [1/62], Loss: 346570.2500
Epoch [18/30], Step [11/62], Loss: 660169.8125
Epoch [18/30], Step [21/62], Loss: 184580.8125
Epoch [18/30], Step [31/62], Loss: 744018.0625
Epoch [18/30], Step [41/62], Loss: 20301.5938
Epoch [18/30], Step [51/62], Loss: 205668.4688
Epoch [18/30], Step [61/62], Loss: 62901.9883
Epoch [19/30], Step [1/62], Loss: 575330.7500
Epoch [19/30], Step [11/62], Loss: 331943.2812
Epoch [19/30], Step [21/62], Loss: 262078.5469
Epoch [19/30], Step [31/62], Loss: 1471819.3750
Epoch [19/30], Step [41/62], Loss: 1332776.1250
Epoch [19/30], Step [51/62], Loss: 1456550.6250
Epoch [19/30], Step [61/62], Loss: 559278.9375
Epoch [20/30], Step [1/62], Loss: 506424.6562
Epoch [20/30], Step [11/62], Loss: 1030272.5000
Epoch [20/30], Step [21/62], Loss: 359629.1562
Epoch [20/30], Step [31/62], Loss: 1477146.1250
Epoch [20/30], Step [41/62], Loss: 373142.0000
Epoch [20/30], Step [51/62], Loss: 920781.3750
Epoch [20/30], Step [61/62], Loss: 171941.7656
Epoch [21/30], Step [1/62], Loss: 500003.8125
Epoch [21/30], Step [11/62], Loss: 1220398.2500
Epoch [21/30], Step [21/62], Loss: 629834.6250
Epoch [21/30], Step [31/62], Loss: 290423.8750
Epoch [21/30], Step [41/62], Loss: 280153.0312
Epoch [21/30], Step [51/62], Loss: 343689.9062
Epoch [21/30], Step [61/62], Loss: 94553.7266
Epoch [22/30], Step [1/62], Loss: 789609.1875
Epoch [22/30], Step [11/62], Loss: 862511.3750
Epoch [22/30], Step [21/62], Loss: 271996.9688
Epoch [22/30], Step [31/62], Loss: 225848.2188
Epoch [22/30], Step [41/62], Loss: 273961.7812
Epoch [22/30], Step [51/62], Loss: 633219.9375
Epoch [22/30], Step [61/62], Loss: 83144.4453
Epoch [23/30], Step [1/62], Loss: 457427.5000
Epoch [23/30], Step [11/62], Loss: 679671.7500
Epoch [23/30], Step [21/62], Loss: 802487.1250
Epoch [23/30], Step [31/62], Loss: 73166.1484
Epoch [23/30], Step [41/62], Loss: 326039.3125
Epoch [23/30], Step [51/62], Loss: 632513.7500
Epoch [23/30], Step [61/62], Loss: 96097.7266
Epoch [24/30], Step [1/62], Loss: 289247.2812
Epoch [24/30], Step [11/62], Loss: 269973.0312
Epoch [24/30], Step [21/62], Loss: 578972.8125
Epoch [24/30], Step [31/62], Loss: 735225.6875
Epoch [24/30], Step [41/62], Loss: 351169.8125
Epoch [24/30], Step [51/62], Loss: 293466.3438
Epoch [24/30], Step [61/62], Loss: 497165.6875
Epoch [25/30], Step [1/62], Loss: 588104.2500
Epoch [25/30], Step [11/62], Loss: 149049.7812
Epoch [25/30], Step [21/62], Loss: 137428.5312
Epoch [25/30], Step [31/62], Loss: 275270.6875
Epoch [25/30], Step [41/62], Loss: 760164.5000
Epoch [25/30], Step [51/62], Loss: 410810.2188
Epoch [25/30], Step [61/62], Loss: 317342.7500
Epoch [26/30], Step [1/62], Loss: 670481.4375
Epoch [26/30], Step [11/62], Loss: 770500.8750
Epoch [26/30], Step [21/62], Loss: 141847.3906
Epoch [26/30], Step [31/62], Loss: 270558.5938
Epoch [26/30], Step [41/62], Loss: 457980.1562
Epoch [26/30], Step [51/62], Loss: 549858.5625
Epoch [26/30], Step [61/62], Loss: 1238046.1250
Epoch [27/30], Step [1/62], Loss: 202805.2969
Epoch [27/30], Step [11/62], Loss: 555116.6250
Epoch [27/30], Step [21/62], Loss: 211204.5469
Epoch [27/30], Step [31/62], Loss: 627606.3750
Epoch [27/30], Step [41/62], Loss: 96791.2812
Epoch [27/30], Step [51/62], Loss: 55250.6602
Epoch [27/30], Step [61/62], Loss: 151618.0156
Epoch [28/30], Step [1/62], Loss: 305469.5000
Epoch [28/30], Step [11/62], Loss: 25395.4062
Epoch [28/30], Step [21/62], Loss: 391026.7500
Epoch [28/30], Step [31/62], Loss: 388726.7188
Epoch [28/30], Step [41/62], Loss: 1834314.8750
Epoch [28/30], Step [51/62], Loss: 110598.5781
Epoch [28/30], Step [61/62], Loss: 196752.0938
Epoch [29/30], Step [1/62], Loss: 9514.8223
Epoch [29/30], Step [11/62], Loss: 840979.5625
Epoch [29/30], Step [21/62], Loss: 422994.3750
Epoch [29/30], Step [31/62], Loss: 400750.6250
Epoch [29/30], Step [41/62], Loss: 530808.0000
Epoch [29/30], Step [51/62], Loss: 226297.8750
Epoch [29/30], Step [61/62], Loss: 780307.7500
Epoch [30/30], Step [1/62], Loss: 12885.5840
Epoch [30/30], Step [11/62], Loss: 1240117.1250
Epoch [30/30], Step [21/62], Loss: 117599.0234
Epoch [30/30], Step [31/62], Loss: 1067579.2500
Epoch [30/30], Step [41/62], Loss: 151769.1719
Epoch [30/30], Step [51/62], Loss: 160097.6094
Epoch [30/30], Step [61/62], Loss: 133852.0156
denselstm(
  (lstm): LSTM(256, 256, batch_first=True)
  (fc): Linear(in_features=6912, out_features=2304, bias=True)
)
Epoch [1/30], Step [1/62], Loss: 0.8016
Epoch [1/30], Step [11/62], Loss: 0.7987
Epoch [1/30], Step [21/62], Loss: 0.7996
Epoch [1/30], Step [31/62], Loss: 0.8036
Epoch [1/30], Step [41/62], Loss: 0.7978
Epoch [1/30], Step [51/62], Loss: 0.8048
Epoch [1/30], Step [61/62], Loss: 0.8003
Epoch [2/30], Step [1/62], Loss: 0.6963
Epoch [2/30], Step [11/62], Loss: 0.6798
Epoch [2/30], Step [21/62], Loss: 0.6766
Epoch [2/30], Step [31/62], Loss: 0.6864
Epoch [2/30], Step [41/62], Loss: 0.6886
Epoch [2/30], Step [51/62], Loss: 0.6935
Epoch [2/30], Step [61/62], Loss: 0.7062
Epoch [3/30], Step [1/62], Loss: 0.5316
Epoch [3/30], Step [11/62], Loss: 0.5237
Epoch [3/30], Step [21/62], Loss: 0.5428
Epoch [3/30], Step [31/62], Loss: 0.5591
Epoch [3/30], Step [41/62], Loss: 0.5684
Epoch [3/30], Step [51/62], Loss: 0.5815
Epoch [3/30], Step [61/62], Loss: 0.5781
Epoch [4/30], Step [1/62], Loss: 0.4309
Epoch [4/30], Step [11/62], Loss: 0.4262
Epoch [4/30], Step [21/62], Loss: 0.4414
Epoch [4/30], Step [31/62], Loss: 0.4492
Epoch [4/30], Step [41/62], Loss: 0.4604
Epoch [4/30], Step [51/62], Loss: 0.4691
Epoch [4/30], Step [61/62], Loss: 0.4794
Epoch [5/30], Step [1/62], Loss: 0.3531
Epoch [5/30], Step [11/62], Loss: 0.3601
Epoch [5/30], Step [21/62], Loss: 0.3670
Epoch [5/30], Step [31/62], Loss: 0.3704
Epoch [5/30], Step [41/62], Loss: 0.3785
Epoch [5/30], Step [51/62], Loss: 0.3872
Epoch [5/30], Step [61/62], Loss: 0.3927
Epoch [6/30], Step [1/62], Loss: 0.3058
Epoch [6/30], Step [11/62], Loss: 0.3034
Epoch [6/30], Step [21/62], Loss: 0.3059
Epoch [6/30], Step [31/62], Loss: 0.3137
Epoch [6/30], Step [41/62], Loss: 0.3206
Epoch [6/30], Step [51/62], Loss: 0.3304
Epoch [6/30], Step [61/62], Loss: 0.3319
Epoch [7/30], Step [1/62], Loss: 0.2599
Epoch [7/30], Step [11/62], Loss: 0.2689
Epoch [7/30], Step [21/62], Loss: 0.2712
Epoch [7/30], Step [31/62], Loss: 0.2740
Epoch [7/30], Step [41/62], Loss: 0.2822
Epoch [7/30], Step [51/62], Loss: 0.2809
Epoch [7/30], Step [61/62], Loss: 0.2835
Epoch [8/30], Step [1/62], Loss: 0.2374
Epoch [8/30], Step [11/62], Loss: 0.2381
Epoch [8/30], Step [21/62], Loss: 0.2394
Epoch [8/30], Step [31/62], Loss: 0.2414
Epoch [8/30], Step [41/62], Loss: 0.2463
Epoch [8/30], Step [51/62], Loss: 0.2456
Epoch [8/30], Step [61/62], Loss: 0.2488
Epoch [9/30], Step [1/62], Loss: 0.2158
Epoch [9/30], Step [11/62], Loss: 0.2177
Epoch [9/30], Step [21/62], Loss: 0.2191
Epoch [9/30], Step [31/62], Loss: 0.2156
Epoch [9/30], Step [41/62], Loss: 0.2222
Epoch [9/30], Step [51/62], Loss: 0.2231
Epoch [9/30], Step [61/62], Loss: 0.2216
Epoch [10/30], Step [1/62], Loss: 0.1966
Epoch [10/30], Step [11/62], Loss: 0.2021
Epoch [10/30], Step [21/62], Loss: 0.2007
Epoch [10/30], Step [31/62], Loss: 0.1994
Epoch [10/30], Step [41/62], Loss: 0.2012
Epoch [10/30], Step [51/62], Loss: 0.2021
Epoch [10/30], Step [61/62], Loss: 0.2054
Epoch [11/30], Step [1/62], Loss: 0.1875
Epoch [11/30], Step [11/62], Loss: 0.1858
Epoch [11/30], Step [21/62], Loss: 0.1865
Epoch [11/30], Step [31/62], Loss: 0.1878
Epoch [11/30], Step [41/62], Loss: 0.1861
Epoch [11/30], Step [51/62], Loss: 0.1885
Epoch [11/30], Step [61/62], Loss: 0.1886
Epoch [12/30], Step [1/62], Loss: 0.1760
Epoch [12/30], Step [11/62], Loss: 0.1747
Epoch [12/30], Step [21/62], Loss: 0.1753
Epoch [12/30], Step [31/62], Loss: 0.1749
Epoch [12/30], Step [41/62], Loss: 0.1755
Epoch [12/30], Step [51/62], Loss: 0.1740
Epoch [12/30], Step [61/62], Loss: 0.1745
Epoch [13/30], Step [1/62], Loss: 0.1668
Epoch [13/30], Step [11/62], Loss: 0.1663
Epoch [13/30], Step [21/62], Loss: 0.1654
Epoch [13/30], Step [31/62], Loss: 0.1652
Epoch [13/30], Step [41/62], Loss: 0.1638
Epoch [13/30], Step [51/62], Loss: 0.1654
Epoch [13/30], Step [61/62], Loss: 0.1657
Epoch [14/30], Step [1/62], Loss: 0.1612
Epoch [14/30], Step [11/62], Loss: 0.1586
Epoch [14/30], Step [21/62], Loss: 0.1587
Epoch [14/30], Step [31/62], Loss: 0.1571
Epoch [14/30], Step [41/62], Loss: 0.1578
Epoch [14/30], Step [51/62], Loss: 0.1554
Epoch [14/30], Step [61/62], Loss: 0.1565
Epoch [15/30], Step [1/62], Loss: 0.1514
Epoch [15/30], Step [11/62], Loss: 0.1540
Epoch [15/30], Step [21/62], Loss: 0.1511
Epoch [15/30], Step [31/62], Loss: 0.1514
Epoch [15/30], Step [41/62], Loss: 0.1514
Epoch [15/30], Step [51/62], Loss: 0.1513
Epoch [15/30], Step [61/62], Loss: 0.1503
Epoch [16/30], Step [1/62], Loss: 0.1499
Epoch [16/30], Step [11/62], Loss: 0.1458
Epoch [16/30], Step [21/62], Loss: 0.1451
Epoch [16/30], Step [31/62], Loss: 0.1450
Epoch [16/30], Step [41/62], Loss: 0.1451
Epoch [16/30], Step [51/62], Loss: 0.1456
Epoch [16/30], Step [61/62], Loss: 0.1451
Epoch [17/30], Step [1/62], Loss: 0.1436
Epoch [17/30], Step [11/62], Loss: 0.1434
Epoch [17/30], Step [21/62], Loss: 0.1416
Epoch [17/30], Step [31/62], Loss: 0.1401
Epoch [17/30], Step [41/62], Loss: 0.1414
Epoch [17/30], Step [51/62], Loss: 0.1402
Epoch [17/30], Step [61/62], Loss: 0.1385
Epoch [18/30], Step [1/62], Loss: 0.1384
Epoch [18/30], Step [11/62], Loss: 0.1399
Epoch [18/30], Step [21/62], Loss: 0.1371
Epoch [18/30], Step [31/62], Loss: 0.1373
Epoch [18/30], Step [41/62], Loss: 0.1370
Epoch [18/30], Step [51/62], Loss: 0.1353
Epoch [18/30], Step [61/62], Loss: 0.1357
Epoch [19/30], Step [1/62], Loss: 0.1383
Epoch [19/30], Step [11/62], Loss: 0.1355
Epoch [19/30], Step [21/62], Loss: 0.1337
Epoch [19/30], Step [31/62], Loss: 0.1339
Epoch [19/30], Step [41/62], Loss: 0.1336
Epoch [19/30], Step [51/62], Loss: 0.1333
Epoch [19/30], Step [61/62], Loss: 0.1325
Epoch [20/30], Step [1/62], Loss: 0.1343
Epoch [20/30], Step [11/62], Loss: 0.1317
Epoch [20/30], Step [21/62], Loss: 0.1311
Epoch [20/30], Step [31/62], Loss: 0.1306
Epoch [20/30], Step [41/62], Loss: 0.1285
Epoch [20/30], Step [51/62], Loss: 0.1282
Epoch [20/30], Step [61/62], Loss: 0.1293
Epoch [21/30], Step [1/62], Loss: 0.1295
Epoch [21/30], Step [11/62], Loss: 0.1294
Epoch [21/30], Step [21/62], Loss: 0.1269
Epoch [21/30], Step [31/62], Loss: 0.1247
Epoch [21/30], Step [41/62], Loss: 0.1262
Epoch [21/30], Step [51/62], Loss: 0.1272
Epoch [21/30], Step [61/62], Loss: 0.1260
Epoch [22/30], Step [1/62], Loss: 0.1265
Epoch [22/30], Step [11/62], Loss: 0.1257
Epoch [22/30], Step [21/62], Loss: 0.1258
Epoch [22/30], Step [31/62], Loss: 0.1251
Epoch [22/30], Step [41/62], Loss: 0.1227
Epoch [22/30], Step [51/62], Loss: 0.1226
Epoch [22/30], Step [61/62], Loss: 0.1238
Epoch [23/30], Step [1/62], Loss: 0.1267
Epoch [23/30], Step [11/62], Loss: 0.1238
Epoch [23/30], Step [21/62], Loss: 0.1221
Epoch [23/30], Step [31/62], Loss: 0.1209
Epoch [23/30], Step [41/62], Loss: 0.1218
Epoch [23/30], Step [51/62], Loss: 0.1195
Epoch [23/30], Step [61/62], Loss: 0.1200
Epoch [24/30], Step [1/62], Loss: 0.1215
Epoch [24/30], Step [11/62], Loss: 0.1220
Epoch [24/30], Step [21/62], Loss: 0.1194
Epoch [24/30], Step [31/62], Loss: 0.1197
Epoch [24/30], Step [41/62], Loss: 0.1191
Epoch [24/30], Step [51/62], Loss: 0.1176
Epoch [24/30], Step [61/62], Loss: 0.1188
Epoch [25/30], Step [1/62], Loss: 0.1228
Epoch [25/30], Step [11/62], Loss: 0.1197
Epoch [25/30], Step [21/62], Loss: 0.1186
Epoch [25/30], Step [31/62], Loss: 0.1173
Epoch [25/30], Step [41/62], Loss: 0.1172
Epoch [25/30], Step [51/62], Loss: 0.1175
Epoch [25/30], Step [61/62], Loss: 0.1165
Epoch [26/30], Step [1/62], Loss: 0.1186
Epoch [26/30], Step [11/62], Loss: 0.1179
Epoch [26/30], Step [21/62], Loss: 0.1171
Epoch [26/30], Step [31/62], Loss: 0.1153
Epoch [26/30], Step [41/62], Loss: 0.1154
Epoch [26/30], Step [51/62], Loss: 0.1143
Epoch [26/30], Step [61/62], Loss: 0.1141
Epoch [27/30], Step [1/62], Loss: 0.1166
Epoch [27/30], Step [11/62], Loss: 0.1170
Epoch [27/30], Step [21/62], Loss: 0.1147
Epoch [27/30], Step [31/62], Loss: 0.1144
Epoch [27/30], Step [41/62], Loss: 0.1134
Epoch [27/30], Step [51/62], Loss: 0.1132
Epoch [27/30], Step [61/62], Loss: 0.1126
Epoch [28/30], Step [1/62], Loss: 0.1138
Epoch [28/30], Step [11/62], Loss: 0.1143
Epoch [28/30], Step [21/62], Loss: 0.1143
Epoch [28/30], Step [31/62], Loss: 0.1121
Epoch [28/30], Step [41/62], Loss: 0.1122
Epoch [28/30], Step [51/62], Loss: 0.1109
Epoch [28/30], Step [61/62], Loss: 0.1109
Epoch [29/30], Step [1/62], Loss: 0.1130
Epoch [29/30], Step [11/62], Loss: 0.1128
Epoch [29/30], Step [21/62], Loss: 0.1108
Epoch [29/30], Step [31/62], Loss: 0.1105
Epoch [29/30], Step [41/62], Loss: 0.1109
Epoch [29/30], Step [51/62], Loss: 0.1096
Epoch [29/30], Step [61/62], Loss: 0.1097
Epoch [30/30], Step [1/62], Loss: 0.1132
Epoch [30/30], Step [11/62], Loss: 0.1124
Epoch [30/30], Step [21/62], Loss: 0.1114
Epoch [30/30], Step [31/62], Loss: 0.1098
Epoch [30/30], Step [41/62], Loss: 0.1081
Epoch [30/30], Step [51/62], Loss: 0.1087
Epoch [30/30], Step [61/62], Loss: 0.1080
