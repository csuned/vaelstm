****Using Fake Training Data with Normal Distribution****
****Using Fake Training Data with Normal Distribution****
BetaVAE(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Linear(in_features=2304, out_features=2304, bias=True)
  (fc_var): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder_input): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (0): ConvTranspose2d(32, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv2d(32, 2, kernel_size=(3, 1), stride=(1, 1), padding=same)
    (4): Tanh()
  )
)
Epoch [1/30], Step [1/62], Loss: 288459424.0000
Epoch [1/30], Step [11/62], Loss: 121291512.0000
Epoch [1/30], Step [21/62], Loss: 31148674.0000
Epoch [1/30], Step [31/62], Loss: 12313067.0000
Epoch [1/30], Step [41/62], Loss: 13464543.0000
Epoch [1/30], Step [51/62], Loss: 13392607.0000
Epoch [1/30], Step [61/62], Loss: 40552576.0000
Epoch [2/30], Step [1/62], Loss: 82803408.0000
Epoch [2/30], Step [11/62], Loss: 3898532.0000
Epoch [2/30], Step [21/62], Loss: 4864190.5000
Epoch [2/30], Step [31/62], Loss: 4300052.5000
Epoch [2/30], Step [41/62], Loss: 10189584.0000
Epoch [2/30], Step [51/62], Loss: 32412998.0000
Epoch [2/30], Step [61/62], Loss: 1365639.3750
Epoch [3/30], Step [1/62], Loss: 1370341.7500
Epoch [3/30], Step [11/62], Loss: 1883802.5000
Epoch [3/30], Step [21/62], Loss: 2286892.2500
Epoch [3/30], Step [31/62], Loss: 1689956.6250
Epoch [3/30], Step [41/62], Loss: 1823790.6250
Epoch [3/30], Step [51/62], Loss: 1694659.7500
Epoch [3/30], Step [61/62], Loss: 1322147.2500
Epoch [4/30], Step [1/62], Loss: 1055494.6250
Epoch [4/30], Step [11/62], Loss: 8009720.0000
Epoch [4/30], Step [21/62], Loss: 1857620.2500
Epoch [4/30], Step [31/62], Loss: 421987.5000
Epoch [4/30], Step [41/62], Loss: 3063522.2500
Epoch [4/30], Step [51/62], Loss: 210401.7969
Epoch [4/30], Step [61/62], Loss: 224354.4688
Epoch [5/30], Step [1/62], Loss: 249339.8906
Epoch [5/30], Step [11/62], Loss: 554927.4375
Epoch [5/30], Step [21/62], Loss: 2559307.0000
Epoch [5/30], Step [31/62], Loss: 413496.2812
Epoch [5/30], Step [41/62], Loss: 322095.5312
Epoch [5/30], Step [51/62], Loss: 32093.3633
Epoch [5/30], Step [61/62], Loss: 6288581.0000
Epoch [6/30], Step [1/62], Loss: 402102.6875
Epoch [6/30], Step [11/62], Loss: 82337.7500
Epoch [6/30], Step [21/62], Loss: 85201.4609
Epoch [6/30], Step [31/62], Loss: 65473.4531
Epoch [6/30], Step [41/62], Loss: 343457.5625
Epoch [6/30], Step [51/62], Loss: 5660346.0000
Epoch [6/30], Step [61/62], Loss: 1407347.8750
Epoch [7/30], Step [1/62], Loss: 376362.8438
Epoch [7/30], Step [11/62], Loss: 765193.0000
Epoch [7/30], Step [21/62], Loss: 3421014.7500
Epoch [7/30], Step [31/62], Loss: 1321349.1250
Epoch [7/30], Step [41/62], Loss: 978455.5625
Epoch [7/30], Step [51/62], Loss: 205650.9375
Epoch [7/30], Step [61/62], Loss: 1396754.7500
Epoch [8/30], Step [1/62], Loss: 624809.1250
Epoch [8/30], Step [11/62], Loss: 1717565.0000
Epoch [8/30], Step [21/62], Loss: 370436.9375
Epoch [8/30], Step [31/62], Loss: 887901.1875
Epoch [8/30], Step [41/62], Loss: 696203.9375
Epoch [8/30], Step [51/62], Loss: 507158.3125
Epoch [8/30], Step [61/62], Loss: 518522.6562
Epoch [9/30], Step [1/62], Loss: 373385.5938
Epoch [9/30], Step [11/62], Loss: 2638264.7500
Epoch [9/30], Step [21/62], Loss: 1202121.0000
Epoch [9/30], Step [31/62], Loss: 1158736.6250
Epoch [9/30], Step [41/62], Loss: 823925.2500
Epoch [9/30], Step [51/62], Loss: 731079.9375
Epoch [9/30], Step [61/62], Loss: 3271377.0000
Epoch [10/30], Step [1/62], Loss: 1822733.0000
Epoch [10/30], Step [11/62], Loss: 599539.1875
Epoch [10/30], Step [21/62], Loss: 91250.9141
Epoch [10/30], Step [31/62], Loss: 222383.2812
Epoch [10/30], Step [41/62], Loss: 156908.4062
Epoch [10/30], Step [51/62], Loss: 214525.1250
Epoch [10/30], Step [61/62], Loss: 428028.7188
Epoch [11/30], Step [1/62], Loss: 418530.5000
Epoch [11/30], Step [11/62], Loss: 338075.6562
Epoch [11/30], Step [21/62], Loss: 712179.8750
Epoch [11/30], Step [31/62], Loss: 207740.7969
Epoch [11/30], Step [41/62], Loss: 297370.9375
Epoch [11/30], Step [51/62], Loss: 1189829.6250
Epoch [11/30], Step [61/62], Loss: 3603986.7500
Epoch [12/30], Step [1/62], Loss: 9053.7246
Epoch [12/30], Step [11/62], Loss: 709299.6875
Epoch [12/30], Step [21/62], Loss: 714169.8750
Epoch [12/30], Step [31/62], Loss: 89160.1016
Epoch [12/30], Step [41/62], Loss: 759701.0000
Epoch [12/30], Step [51/62], Loss: 1041938.0625
Epoch [12/30], Step [61/62], Loss: 1425572.0000
Epoch [13/30], Step [1/62], Loss: 655252.6875
Epoch [13/30], Step [11/62], Loss: 401027.5938
Epoch [13/30], Step [21/62], Loss: 2332962.0000
Epoch [13/30], Step [31/62], Loss: 1376612.6250
Epoch [13/30], Step [41/62], Loss: 373391.5312
Epoch [13/30], Step [51/62], Loss: 88269.4844
Epoch [13/30], Step [61/62], Loss: 487886.6875
Epoch [14/30], Step [1/62], Loss: 675843.0000
Epoch [14/30], Step [11/62], Loss: 54982.6797
Epoch [14/30], Step [21/62], Loss: 2087520.3750
Epoch [14/30], Step [31/62], Loss: 744883.5625
Epoch [14/30], Step [41/62], Loss: 837641.2500
Epoch [14/30], Step [51/62], Loss: 2127199.2500
Epoch [14/30], Step [61/62], Loss: 1869141.6250
Epoch [15/30], Step [1/62], Loss: 1272653.7500
Epoch [15/30], Step [11/62], Loss: 923386.8125
Epoch [15/30], Step [21/62], Loss: 845850.4375
Epoch [15/30], Step [31/62], Loss: 644122.6250
Epoch [15/30], Step [41/62], Loss: 1146062.2500
Epoch [15/30], Step [51/62], Loss: 980256.8125
Epoch [15/30], Step [61/62], Loss: 1068974.7500
Epoch [16/30], Step [1/62], Loss: 1714040.2500
Epoch [16/30], Step [11/62], Loss: 1405162.6250
Epoch [16/30], Step [21/62], Loss: 495946.3125
Epoch [16/30], Step [31/62], Loss: 729322.6875
Epoch [16/30], Step [41/62], Loss: 1195373.5000
Epoch [16/30], Step [51/62], Loss: 972850.3125
Epoch [16/30], Step [61/62], Loss: 1373338.2500
Epoch [17/30], Step [1/62], Loss: 631400.1250
Epoch [17/30], Step [11/62], Loss: 3309290.7500
Epoch [17/30], Step [21/62], Loss: 375233.4375
Epoch [17/30], Step [31/62], Loss: 763373.1250
Epoch [17/30], Step [41/62], Loss: 6086909.0000
Epoch [17/30], Step [51/62], Loss: 1225273.8750
Epoch [17/30], Step [61/62], Loss: 722888.2500
Epoch [18/30], Step [1/62], Loss: 1761339.8750
Epoch [18/30], Step [11/62], Loss: 7916.4678
Epoch [18/30], Step [21/62], Loss: 4213361.5000
Epoch [18/30], Step [31/62], Loss: 795888.1875
Epoch [18/30], Step [41/62], Loss: 18305.0801
Epoch [18/30], Step [51/62], Loss: 738338.6875
Epoch [18/30], Step [61/62], Loss: 228927.1562
Epoch [19/30], Step [1/62], Loss: 579433.1875
Epoch [19/30], Step [11/62], Loss: 797305.1250
Epoch [19/30], Step [21/62], Loss: 70297.4922
Epoch [19/30], Step [31/62], Loss: 1238826.2500
Epoch [19/30], Step [41/62], Loss: 482603.0938
Epoch [19/30], Step [51/62], Loss: 1107202.0000
Epoch [19/30], Step [61/62], Loss: 300933.6250
Epoch [20/30], Step [1/62], Loss: 1656311.7500
Epoch [20/30], Step [11/62], Loss: 841366.0625
Epoch [20/30], Step [21/62], Loss: 21397.1309
Epoch [20/30], Step [31/62], Loss: 219112.4062
Epoch [20/30], Step [41/62], Loss: 364445.9688
Epoch [20/30], Step [51/62], Loss: 888668.3125
Epoch [20/30], Step [61/62], Loss: 41083.8281
Epoch [21/30], Step [1/62], Loss: 1061076.6250
Epoch [21/30], Step [11/62], Loss: 53248.1836
Epoch [21/30], Step [21/62], Loss: 3490515.5000
Epoch [21/30], Step [31/62], Loss: 569491.1250
Epoch [21/30], Step [41/62], Loss: 68244.4766
Epoch [21/30], Step [51/62], Loss: 765258.3125
Epoch [21/30], Step [61/62], Loss: 783197.6250
Epoch [22/30], Step [1/62], Loss: 871406.5625
Epoch [22/30], Step [11/62], Loss: 998412.3750
Epoch [22/30], Step [21/62], Loss: 205182.0938
Epoch [22/30], Step [31/62], Loss: 753168.3750
Epoch [22/30], Step [41/62], Loss: 1102440.6250
Epoch [22/30], Step [51/62], Loss: 1353144.8750
Epoch [22/30], Step [61/62], Loss: 215243.5938
Epoch [23/30], Step [1/62], Loss: 160480.7500
Epoch [23/30], Step [11/62], Loss: 687682.1250
Epoch [23/30], Step [21/62], Loss: 526776.0625
Epoch [23/30], Step [31/62], Loss: 646118.3750
Epoch [23/30], Step [41/62], Loss: 868736.2500
Epoch [23/30], Step [51/62], Loss: 201307.3125
Epoch [23/30], Step [61/62], Loss: 821353.4375
Epoch [24/30], Step [1/62], Loss: 1644980.6250
Epoch [24/30], Step [11/62], Loss: 204700.9688
Epoch [24/30], Step [21/62], Loss: 123647.7031
Epoch [24/30], Step [31/62], Loss: 410595.4375
Epoch [24/30], Step [41/62], Loss: 529951.5625
Epoch [24/30], Step [51/62], Loss: 788263.0625
Epoch [24/30], Step [61/62], Loss: 515806.0000
Epoch [25/30], Step [1/62], Loss: 554318.4375
Epoch [25/30], Step [11/62], Loss: 353476.3125
Epoch [25/30], Step [21/62], Loss: 45122.6406
Epoch [25/30], Step [31/62], Loss: 784758.3125
Epoch [25/30], Step [41/62], Loss: 407710.3438
Epoch [25/30], Step [51/62], Loss: 910317.8750
Epoch [25/30], Step [61/62], Loss: 1841097.3750
Epoch [26/30], Step [1/62], Loss: 241988.4375
Epoch [26/30], Step [11/62], Loss: 587370.6250
Epoch [26/30], Step [21/62], Loss: 614905.1250
Epoch [26/30], Step [31/62], Loss: 655243.8750
Epoch [26/30], Step [41/62], Loss: 882015.2500
Epoch [26/30], Step [51/62], Loss: 1542663.8750
Epoch [26/30], Step [61/62], Loss: 312679.5625
Epoch [27/30], Step [1/62], Loss: 748143.1875
Epoch [27/30], Step [11/62], Loss: 316649.9375
Epoch [27/30], Step [21/62], Loss: 1228135.1250
Epoch [27/30], Step [31/62], Loss: 1135050.2500
Epoch [27/30], Step [41/62], Loss: 81194.8906
Epoch [27/30], Step [51/62], Loss: 1380344.8750
Epoch [27/30], Step [61/62], Loss: 179651.7500
Epoch [28/30], Step [1/62], Loss: 834043.5000
Epoch [28/30], Step [11/62], Loss: 521668.0000
Epoch [28/30], Step [21/62], Loss: 50609.6055
Epoch [28/30], Step [31/62], Loss: 799191.0000
Epoch [28/30], Step [41/62], Loss: 956457.6250
Epoch [28/30], Step [51/62], Loss: 397318.4062
Epoch [28/30], Step [61/62], Loss: 1020268.9375
Epoch [29/30], Step [1/62], Loss: 683882.6875
Epoch [29/30], Step [11/62], Loss: 331854.3750
Epoch [29/30], Step [21/62], Loss: 528802.8750
Epoch [29/30], Step [31/62], Loss: 566854.9375
Epoch [29/30], Step [41/62], Loss: 394954.7188
Epoch [29/30], Step [51/62], Loss: 1182680.6250
Epoch [29/30], Step [61/62], Loss: 262382.0625
Epoch [30/30], Step [1/62], Loss: 233404.6562
Epoch [30/30], Step [11/62], Loss: 314995.8125
Epoch [30/30], Step [21/62], Loss: 1174202.8750
Epoch [30/30], Step [31/62], Loss: 949517.2500
Epoch [30/30], Step [41/62], Loss: 503479.5000
Epoch [30/30], Step [51/62], Loss: 1545892.2500
Epoch [30/30], Step [61/62], Loss: 459892.3125
denselstm(
  (lstm): LSTM(256, 256, batch_first=True)
  (fc): Linear(in_features=6912, out_features=2304, bias=True)
)
Epoch [1/30], Step [1/62], Loss: 0.8003
Epoch [1/30], Step [11/62], Loss: 0.8023
Epoch [1/30], Step [21/62], Loss: 0.8037
Epoch [1/30], Step [31/62], Loss: 0.8004
Epoch [1/30], Step [41/62], Loss: 0.8019
Epoch [1/30], Step [51/62], Loss: 0.8029
Epoch [1/30], Step [61/62], Loss: 0.8062
Epoch [2/30], Step [1/62], Loss: 0.6945
Epoch [2/30], Step [11/62], Loss: 0.6800
Epoch [2/30], Step [21/62], Loss: 0.6815
Epoch [2/30], Step [31/62], Loss: 0.6794
Epoch [2/30], Step [41/62], Loss: 0.6823
Epoch [2/30], Step [51/62], Loss: 0.6977
Epoch [2/30], Step [61/62], Loss: 0.7038
Epoch [3/30], Step [1/62], Loss: 0.5382
Epoch [3/30], Step [11/62], Loss: 0.5423
Epoch [3/30], Step [21/62], Loss: 0.5417
Epoch [3/30], Step [31/62], Loss: 0.5668
Epoch [3/30], Step [41/62], Loss: 0.5748
Epoch [3/30], Step [51/62], Loss: 0.5762
Epoch [3/30], Step [61/62], Loss: 0.5781
Epoch [4/30], Step [1/62], Loss: 0.4278
Epoch [4/30], Step [11/62], Loss: 0.4386
Epoch [4/30], Step [21/62], Loss: 0.4394
Epoch [4/30], Step [31/62], Loss: 0.4537
Epoch [4/30], Step [41/62], Loss: 0.4551
Epoch [4/30], Step [51/62], Loss: 0.4720
Epoch [4/30], Step [61/62], Loss: 0.4749
Epoch [5/30], Step [1/62], Loss: 0.3477
Epoch [5/30], Step [11/62], Loss: 0.3581
Epoch [5/30], Step [21/62], Loss: 0.3681
Epoch [5/30], Step [31/62], Loss: 0.3719
Epoch [5/30], Step [41/62], Loss: 0.3785
Epoch [5/30], Step [51/62], Loss: 0.3897
Epoch [5/30], Step [61/62], Loss: 0.3928
Epoch [6/30], Step [1/62], Loss: 0.3023
Epoch [6/30], Step [11/62], Loss: 0.3036
Epoch [6/30], Step [21/62], Loss: 0.3150
Epoch [6/30], Step [31/62], Loss: 0.3137
Epoch [6/30], Step [41/62], Loss: 0.3203
Epoch [6/30], Step [51/62], Loss: 0.3279
Epoch [6/30], Step [61/62], Loss: 0.3322
Epoch [7/30], Step [1/62], Loss: 0.2662
Epoch [7/30], Step [11/62], Loss: 0.2685
Epoch [7/30], Step [21/62], Loss: 0.2664
Epoch [7/30], Step [31/62], Loss: 0.2787
Epoch [7/30], Step [41/62], Loss: 0.2818
Epoch [7/30], Step [51/62], Loss: 0.2830
Epoch [7/30], Step [61/62], Loss: 0.2824
Epoch [8/30], Step [1/62], Loss: 0.2371
Epoch [8/30], Step [11/62], Loss: 0.2430
Epoch [8/30], Step [21/62], Loss: 0.2412
Epoch [8/30], Step [31/62], Loss: 0.2432
Epoch [8/30], Step [41/62], Loss: 0.2477
Epoch [8/30], Step [51/62], Loss: 0.2463
Epoch [8/30], Step [61/62], Loss: 0.2518
Epoch [9/30], Step [1/62], Loss: 0.2139
Epoch [9/30], Step [11/62], Loss: 0.2166
Epoch [9/30], Step [21/62], Loss: 0.2170
Epoch [9/30], Step [31/62], Loss: 0.2184
Epoch [9/30], Step [41/62], Loss: 0.2175
Epoch [9/30], Step [51/62], Loss: 0.2235
Epoch [9/30], Step [61/62], Loss: 0.2254
Epoch [10/30], Step [1/62], Loss: 0.1999
Epoch [10/30], Step [11/62], Loss: 0.1999
Epoch [10/30], Step [21/62], Loss: 0.1984
Epoch [10/30], Step [31/62], Loss: 0.1990
Epoch [10/30], Step [41/62], Loss: 0.2013
Epoch [10/30], Step [51/62], Loss: 0.2035
Epoch [10/30], Step [61/62], Loss: 0.2051
Epoch [11/30], Step [1/62], Loss: 0.1861
Epoch [11/30], Step [11/62], Loss: 0.1866
Epoch [11/30], Step [21/62], Loss: 0.1882
Epoch [11/30], Step [31/62], Loss: 0.1865
Epoch [11/30], Step [41/62], Loss: 0.1861
Epoch [11/30], Step [51/62], Loss: 0.1854
Epoch [11/30], Step [61/62], Loss: 0.1884
Epoch [12/30], Step [1/62], Loss: 0.1748
Epoch [12/30], Step [11/62], Loss: 0.1760
Epoch [12/30], Step [21/62], Loss: 0.1764
Epoch [12/30], Step [31/62], Loss: 0.1746
Epoch [12/30], Step [41/62], Loss: 0.1760
Epoch [12/30], Step [51/62], Loss: 0.1756
Epoch [12/30], Step [61/62], Loss: 0.1766
Epoch [13/30], Step [1/62], Loss: 0.1673
Epoch [13/30], Step [11/62], Loss: 0.1670
Epoch [13/30], Step [21/62], Loss: 0.1666
Epoch [13/30], Step [31/62], Loss: 0.1649
Epoch [13/30], Step [41/62], Loss: 0.1663
Epoch [13/30], Step [51/62], Loss: 0.1659
Epoch [13/30], Step [61/62], Loss: 0.1651
Epoch [14/30], Step [1/62], Loss: 0.1621
Epoch [14/30], Step [11/62], Loss: 0.1614
Epoch [14/30], Step [21/62], Loss: 0.1582
Epoch [14/30], Step [31/62], Loss: 0.1582
Epoch [14/30], Step [41/62], Loss: 0.1574
Epoch [14/30], Step [51/62], Loss: 0.1569
Epoch [14/30], Step [61/62], Loss: 0.1576
Epoch [15/30], Step [1/62], Loss: 0.1543
Epoch [15/30], Step [11/62], Loss: 0.1542
Epoch [15/30], Step [21/62], Loss: 0.1532
Epoch [15/30], Step [31/62], Loss: 0.1512
Epoch [15/30], Step [41/62], Loss: 0.1508
Epoch [15/30], Step [51/62], Loss: 0.1524
Epoch [15/30], Step [61/62], Loss: 0.1507
Epoch [16/30], Step [1/62], Loss: 0.1489
Epoch [16/30], Step [11/62], Loss: 0.1498
Epoch [16/30], Step [21/62], Loss: 0.1461
Epoch [16/30], Step [31/62], Loss: 0.1452
Epoch [16/30], Step [41/62], Loss: 0.1452
Epoch [16/30], Step [51/62], Loss: 0.1464
Epoch [16/30], Step [61/62], Loss: 0.1455
Epoch [17/30], Step [1/62], Loss: 0.1429
Epoch [17/30], Step [11/62], Loss: 0.1423
Epoch [17/30], Step [21/62], Loss: 0.1437
Epoch [17/30], Step [31/62], Loss: 0.1413
Epoch [17/30], Step [41/62], Loss: 0.1405
Epoch [17/30], Step [51/62], Loss: 0.1393
Epoch [17/30], Step [61/62], Loss: 0.1398
Epoch [18/30], Step [1/62], Loss: 0.1402
Epoch [18/30], Step [11/62], Loss: 0.1387
Epoch [18/30], Step [21/62], Loss: 0.1383
Epoch [18/30], Step [31/62], Loss: 0.1372
Epoch [18/30], Step [41/62], Loss: 0.1354
Epoch [18/30], Step [51/62], Loss: 0.1357
Epoch [18/30], Step [61/62], Loss: 0.1363
Epoch [19/30], Step [1/62], Loss: 0.1371
Epoch [19/30], Step [11/62], Loss: 0.1356
Epoch [19/30], Step [21/62], Loss: 0.1339
Epoch [19/30], Step [31/62], Loss: 0.1343
Epoch [19/30], Step [41/62], Loss: 0.1327
Epoch [19/30], Step [51/62], Loss: 0.1315
Epoch [19/30], Step [61/62], Loss: 0.1314
Epoch [20/30], Step [1/62], Loss: 0.1344
Epoch [20/30], Step [11/62], Loss: 0.1325
Epoch [20/30], Step [21/62], Loss: 0.1309
Epoch [20/30], Step [31/62], Loss: 0.1297
Epoch [20/30], Step [41/62], Loss: 0.1285
Epoch [20/30], Step [51/62], Loss: 0.1285
Epoch [20/30], Step [61/62], Loss: 0.1285
Epoch [21/30], Step [1/62], Loss: 0.1291
Epoch [21/30], Step [11/62], Loss: 0.1309
Epoch [21/30], Step [21/62], Loss: 0.1278
Epoch [21/30], Step [31/62], Loss: 0.1264
Epoch [21/30], Step [41/62], Loss: 0.1264
Epoch [21/30], Step [51/62], Loss: 0.1262
Epoch [21/30], Step [61/62], Loss: 0.1251
Epoch [22/30], Step [1/62], Loss: 0.1297
Epoch [22/30], Step [11/62], Loss: 0.1270
Epoch [22/30], Step [21/62], Loss: 0.1252
Epoch [22/30], Step [31/62], Loss: 0.1254
Epoch [22/30], Step [41/62], Loss: 0.1235
Epoch [22/30], Step [51/62], Loss: 0.1222
Epoch [22/30], Step [61/62], Loss: 0.1224
Epoch [23/30], Step [1/62], Loss: 0.1245
Epoch [23/30], Step [11/62], Loss: 0.1225
Epoch [23/30], Step [21/62], Loss: 0.1230
Epoch [23/30], Step [31/62], Loss: 0.1214
Epoch [23/30], Step [41/62], Loss: 0.1226
Epoch [23/30], Step [51/62], Loss: 0.1212
Epoch [23/30], Step [61/62], Loss: 0.1207
Epoch [24/30], Step [1/62], Loss: 0.1207
Epoch [24/30], Step [11/62], Loss: 0.1228
Epoch [24/30], Step [21/62], Loss: 0.1208
Epoch [24/30], Step [31/62], Loss: 0.1198
Epoch [24/30], Step [41/62], Loss: 0.1198
Epoch [24/30], Step [51/62], Loss: 0.1190
Epoch [24/30], Step [61/62], Loss: 0.1187
Epoch [25/30], Step [1/62], Loss: 0.1176
Epoch [25/30], Step [11/62], Loss: 0.1195
Epoch [25/30], Step [21/62], Loss: 0.1200
Epoch [25/30], Step [31/62], Loss: 0.1183
Epoch [25/30], Step [41/62], Loss: 0.1176
Epoch [25/30], Step [51/62], Loss: 0.1166
Epoch [25/30], Step [61/62], Loss: 0.1176
Epoch [26/30], Step [1/62], Loss: 0.1189
Epoch [26/30], Step [11/62], Loss: 0.1190
Epoch [26/30], Step [21/62], Loss: 0.1170
Epoch [26/30], Step [31/62], Loss: 0.1177
Epoch [26/30], Step [41/62], Loss: 0.1156
Epoch [26/30], Step [51/62], Loss: 0.1148
Epoch [26/30], Step [61/62], Loss: 0.1153
Epoch [27/30], Step [1/62], Loss: 0.1169
Epoch [27/30], Step [11/62], Loss: 0.1173
Epoch [27/30], Step [21/62], Loss: 0.1153
Epoch [27/30], Step [31/62], Loss: 0.1128
Epoch [27/30], Step [41/62], Loss: 0.1135
Epoch [27/30], Step [51/62], Loss: 0.1120
Epoch [27/30], Step [61/62], Loss: 0.1131
Epoch [28/30], Step [1/62], Loss: 0.1177
Epoch [28/30], Step [11/62], Loss: 0.1139
Epoch [28/30], Step [21/62], Loss: 0.1136
Epoch [28/30], Step [31/62], Loss: 0.1112
Epoch [28/30], Step [41/62], Loss: 0.1122
Epoch [28/30], Step [51/62], Loss: 0.1126
Epoch [28/30], Step [61/62], Loss: 0.1113
Epoch [29/30], Step [1/62], Loss: 0.1145
Epoch [29/30], Step [11/62], Loss: 0.1118
Epoch [29/30], Step [21/62], Loss: 0.1113
Epoch [29/30], Step [31/62], Loss: 0.1102
Epoch [29/30], Step [41/62], Loss: 0.1108
Epoch [29/30], Step [51/62], Loss: 0.1103
Epoch [29/30], Step [61/62], Loss: 0.1101
Epoch [30/30], Step [1/62], Loss: 0.1121
Epoch [30/30], Step [11/62], Loss: 0.1106
Epoch [30/30], Step [21/62], Loss: 0.1109
Epoch [30/30], Step [31/62], Loss: 0.1093
Epoch [30/30], Step [41/62], Loss: 0.1084
Epoch [30/30], Step [51/62], Loss: 0.1078
Epoch [30/30], Step [61/62], Loss: 0.1085
