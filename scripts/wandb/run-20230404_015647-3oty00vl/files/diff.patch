diff --git a/README.md b/README.md
index cb11ec2..b645b5e 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,3 @@
 # vaelstm
 This is a Torch version of "https://github.com/lin-shuyu/VAE-LSTM-for-anomaly-detection"
+
diff --git a/scripts/run_vae_lstm.py b/scripts/run_vae_lstm.py
index e69de29..82432ee 100644
--- a/scripts/run_vae_lstm.py
+++ b/scripts/run_vae_lstm.py
@@ -0,0 +1,89 @@
+import os, sys
+ROOT_DIR = '/home/jupyter-chuanhao/data/chuanhao_anomaly/vaelstm/'
+SRC_DIR = ROOT_DIR + 'src'
+UTILS_DIR = ROOT_DIR + 'utils'
+sys.path.insert(0, SRC_DIR)
+sys.path.insert(0, UTILS_DIR)
+from vaelstm import MyLSTMVAE
+from myplot import plt_compare_series
+from loaders import vae_training_data
+import torch as pt
+import wandb
+import pdb
+
+
+'''
+Define Global Const
+code_config: per code length, number of code used as input for prediction, dimension of code (2 for b_vae)
+data_train, data_test: training set and evaluation set, add validation set if you want
+in_channels, latent_dim, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type: hyper parameters of VAE
+input_size, hidden_size, latent_size, num_layers: hyper parameters of LSTM
+learning_rate=1e-4, batch_size=32, num_epochs=100, device='cuda0': default traning configs
+'''
+code_config = {
+    'l_seq':144, # this field should be aligned with your VAE latent length
+    'code_size':256, # 2 for the VAE of this template
+    'num_in':3, # how many embeddings to predict next embedding
+    'hidden_len':9,
+    }
+
+in_channels=2
+input_size=256
+hidden_size=256
+latent_size=input_size #LSTM does not change the dimension
+num_layers=1
+latent_dim=2
+hidden_dims=None #using default value
+beta = 4 
+gamma = 1000.
+max_capacity = 25
+Capacity_max_iter = 1e4
+loss_type = 'B'
+seq_len = code_config['l_seq']
+learning_rate = 1e-3
+batch_size=32
+#total_batch #need this to compute KLD weight
+num_epochs=30
+gpu_id = 0 
+device = pt.device(f"cuda:{gpu_id}" if (pt.cuda.is_available() and gpu_id in [0,1]) else "cpu")
+
+
+wandb.login()
+run = wandb.init(
+# Set the project where this run will be logged
+project="MyVAELSTM",
+# Track hyperparameters and run metadata
+config={
+    "learning_rate": learning_rate,
+    "epochs": num_epochs,
+    'VAE_loss_type': 'B',
+})
+
+'''
+Prepare training/eval data
+'''
+#debuging with random input
+data_train, total_batch = vae_training_data(seq_len, in_channel=in_channels)
+data_test, _ = vae_training_data(seq_len, in_channel=in_channels)
+
+'''
+Model Training and Evaluation
+'''
+detector = MyLSTMVAE(data_train, data_test, code_config, total_batch, in_channels, latent_dim, input_size, hidden_size, latent_size, num_layers, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type, seq_len, learning_rate, batch_size, num_epochs, device).to(device)
+#train VAE
+loss_vae_train, input_train, reconst_train, loss_vae_test, input_test, reconst_test = detector.train_vae()
+#train LSTM
+gt_train, gt_test = detector.embedding()
+loss_lstm_train, input_lstm_train, pred_lstm_train, loss_lstm_test, input_lstm_test, pred_lstm_test = detector.train_lstm()
+#Carry out anomaly detection
+reconst = detector.anomaly_detect(pred_lstm_test)
+
+'''
+Plot, local result see results (only save latest result)
+'''
+plt_compare_series([gt_test[:3], reconst[:3]], 'Predictions', label_list=['Ground Truth', 'Predictions'])
+
+run.finish()
+
+
+
diff --git a/scripts/trainer.py b/scripts/trainer.py
index 2e2e5af..a1ca347 100644
--- a/scripts/trainer.py
+++ b/scripts/trainer.py
@@ -1,5 +1,5 @@
 import os, sys
-ROOT_DIR = '//'
+ROOT_DIR = '/home/jupyter-chuanhao/data/chuanhao_anomaly/vaelstm/'
 SRC_DIR = ROOT_DIR + 'src'
 UTILS_DIR = ROOT_DIR + 'utils'
 sys.path.insert(0, SRC_DIR)
@@ -8,15 +8,16 @@ import torch as pt
 import torch.nn as nn
 import torch.nn.functional as func
 from torch.autograd import Variable
-from src.model.vae import BetaVAE
-from src.model.rnns import denselstm
-from utils.loaders import anomaly_loader, embedding_loader
+from models.vae import BetaVAE
+from models.rnns import denselstm
+from loaders import anomaly_loader, embedding_loader
 from torch.utils.data import DataLoader
+import pdb
+import wandb
 
 class MyVAE(nn.Module):
 
-    def __init__(self, data_train, in_channels: int, latent_dim: int, hidden_dims = None, beta: int = 4, gamma: float = 1000., max_capacity: int = 25,
-                 Capacity_max_iter: int = 1e4, loss_type: str = 'B', learning_rate=1e-4, batch_size=32) -> None:
+    def __init__(self, data_train, in_channels: int, latent_dim: int, batchsize:int, total_batch: int, hidden_dims = None, beta: int = 4, gamma: float = 1000., max_capacity: int = 25, Capacity_max_iter: int = 1e4, loss_type: str = 'B', learning_rate=1e-4, batch_size=32, seq_len: int=48, device='cuda:0', use_wandb=True) -> None:
         super(MyVAE, self).__init__()
 
         self.in_channels = in_channels
@@ -27,10 +28,11 @@ class MyVAE(nn.Module):
         self.C_stop_iter = Capacity_max_iter
         self.loss_type = loss_type
         self.learning_rate = learning_rate
+        self.use_wandb=use_wandb
 
-        self.b_vae = BetaVAE(in_channels, latent_dim, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type)
+        self.b_vae = BetaVAE(in_channels, latent_dim, batchsize, total_batch, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type, seq_len)
 
-        self.device = pt.device('cuda' if pt.cuda.is_available() else 'cpu')
+        self.device = device
         self.optimizer = pt.optim.Adam(self.b_vae.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=0)
         loader = anomaly_loader(data_train)
         self.dataloader = DataLoader(loader, batch_size=batch_size, shuffle=True, drop_last=True)
@@ -39,16 +41,18 @@ class MyVAE(nn.Module):
     def train(self, num_epochs=100):
         self.b_vae.to(self.device)
         for epoch in range(num_epochs):
-            for i, (x, _) in enumerate(self.dataloader):
+            wandb.log({"epoch_vae": epoch + 1})
+            for i, x in enumerate(self.dataloader):
                 x = x.to(self.device)
                 self.optimizer.zero_grad()
                 recon_x, _, mu, logvar = self.b_vae(x)
                 loss = self.b_vae.loss_function(recon_x, x, mu, logvar)
                 loss['loss'].backward()
                 self.optimizer.step()
+                wandb.log({"KLD": loss['KLD'].item(),"accuracy": loss['Reconstruction_Loss'].item(), "loss_vae": loss['loss'].item()})
                 if i % 10 == 0:
                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
-                          .format(epoch + 1, num_epochs, i + 1, len(self.dataloader), loss.item()))
+                          .format(epoch + 1, num_epochs, i + 1, len(self.dataloader), loss['loss'].item()))
         pt.save(self.b_vae.state_dict(), 'vae.pth')
         return self.b_vae
 
@@ -59,7 +63,7 @@ class MyVAE(nn.Module):
         self.b_vae.eval()
         loader = anomaly_loader(data_test)
         eval_dataloader = DataLoader(loader, batch_size=1, shuffle=True, drop_last=True)
-        for i, (x, _) in enumerate(eval_dataloader):
+        for i, x in enumerate(eval_dataloader):
             x = x.to(self.device)
             recon_x, _, mu, logvar = self.b_vae(x)
             loss = self.b_vae.loss_function(recon_x, x, mu, logvar)
@@ -78,6 +82,7 @@ class MyVAE(nn.Module):
         self.b_vae.to(self.device)
         self.b_vae.eval()
         return
+    
     def encode(self, x):
         return self.b_vae.encode(x)
 
@@ -85,14 +90,15 @@ class MyVAE(nn.Module):
         return self.b_vae.decode(z)
 
 class MyLSTM(nn.Module):
-    def __init__(self, data_train, input_size, hidden_size, latent_size, num_layers=1, learning_rate=1e-4, batch_size=32):
+    
+    def __init__(self, data_train, input_size, hidden_size, latent_size, code_config, num_layers=1, learning_rate=1e-4, batch_size=32, device='cuda:0'):
         super(MyLSTM, self).__init__()
         self.input_size = input_size
         self.hidden_size = hidden_size
         self.latent_size = latent_size
         self.num_layers = num_layers
-        self.prednet = denselstm(input_size, hidden_size, num_layers) #prednet: prediction network
-        self.device = pt.device('cuda' if pt.cuda.is_available() else 'cpu')
+        self.prednet = denselstm(input_size, hidden_size, latent_size, num_layers, code_config) #prednet: prediction network
+        self.device = device
         self.optimizer = pt.optim.Adam(self.prednet.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=0)
         loader = embedding_loader(data_train)
         self.dataloader = DataLoader(loader, batch_size=batch_size, shuffle=True, drop_last=True)
@@ -102,13 +108,16 @@ class MyLSTM(nn.Module):
     def train(self, num_epochs=100):
         self.prednet.to(self.device)
         for epoch in range(num_epochs):
-            for i, (x, _) in enumerate(self.dataloader):
-                x = x.to(self.device)
+            wandb.log({"epoch_lstm": epoch + 1})
+            for i, d in enumerate(self.dataloader):
+                x = d['input'].to(self.device).detach()
+                p = d['output'].to(self.device).detach()
                 self.optimizer.zero_grad()
-                y = self.prednet(x)
-                loss = self.criteria(y, x)
+                y = self.prednet(x, self.device)
+                loss = self.criteria(y, p)
                 loss.backward()
                 self.optimizer.step()
+                wandb.log({"loss_lstm": loss.item()})
                 if i % 10 == 0:
                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                           .format(epoch + 1, num_epochs, i + 1, len(self.dataloader), loss.item()))
@@ -120,21 +129,22 @@ class MyLSTM(nn.Module):
             self.prednet.load_state_dict(pt.load('prednet.pth'))
             self.prednet.to(self.device)
         self.prednet.eval()
-        loader = anomaly_loader(data_test)
+        loader = embedding_loader(data_test)
         eval_dataloader = DataLoader(loader, batch_size=1, shuffle=True, drop_last=True)
-        for i, (x, _) in enumerate(eval_dataloader):
-            x = x.to(self.device)
-            y = self.prednet(x)
-            loss = self.criteria(y, x)
+        for i, d in enumerate(eval_dataloader):
+            x = d['input'].to(self.device).detach()
+            p = d['output'].to(self.device).detach()
+            y = self.prednet(x, self.device)
+            loss = self.criteria(y, p)
             if i>0:
-                out_loss = pt.cat((out_loss, loss), 0)
-                out_x = pt.cat((out_x, x), 0)
+                out_loss += [loss.item(),]
+                out_p = pt.cat((out_p, p), 0)
                 out_y = pt.cat((out_y, y), 0)
             else:
-                out_loss = loss
-                out_x = x
+                out_loss = [loss.item(),]
+                out_p = p
                 out_y = y
-        return out_loss.cpu().detach().numpy(), out_x.cpu().detach().numpy(), out_y.cpu().detach().numpy()
+        return out_loss, out_p.cpu().detach().numpy(), out_y.cpu().detach().numpy()
 
     def load_model(self, path):
         self.prednet.load_state_dict(pt.load(path))
diff --git a/scripts/vaelstm.py b/scripts/vaelstm.py
index 36c82b2..a4fd6b4 100644
--- a/scripts/vaelstm.py
+++ b/scripts/vaelstm.py
@@ -1,5 +1,6 @@
 import os, sys
-ROOT_DIR = '//'
+# ROOT_DIR = '/home/jupyter-chuanhao/data/chuanhao_anomaly/vaelstm/'
+ROOT_DIR = '../'
 SRC_DIR = ROOT_DIR + 'src'
 UTILS_DIR = ROOT_DIR + 'utils'
 sys.path.insert(0, SRC_DIR)
@@ -9,14 +10,15 @@ import torch.nn as nn
 import numpy as np
 from trainer import MyVAE
 from trainer import MyLSTM
-from utils.loaders import produce_embeddings, produce_outputs, produce_predicts, anomaly_loader
+from loaders import produce_embeddings, produce_outputs, produce_predicts, anomaly_loader
 from torch.utils.data import DataLoader
-
+from torch.autograd import Variable
+import pdb
 
 class MyLSTMVAE(nn.Module):
-    def __init__(self, data_train, data_test, code_config, in_channels: int, latent_dim: int, input_size: int, hidden_size:int, latent_size:int, num_layers:int,
-                 hidden_dims = None, beta: int = 4, gamma: float = 1000., max_capacity: int = 25,
-                 Capacity_max_iter: int = 1e4, loss_type: str = 'B', learning_rate=1e-4, batch_size=32, num_epochs=100) -> None:
+    
+    def __init__(self, data_train, data_test, code_config, total_batch, in_channels: int, latent_dim: int, input_size: int, hidden_size:int, latent_size:int, num_layers:int, hidden_dims = None, beta: int = 4, gamma: float = 1000., max_capacity: int = 25,
+Capacity_max_iter: int = 1e4, loss_type: str = 'B', seq_len=48, learning_rate=1e-4, batch_size=32, num_epochs=100, device='cuda0') -> None:
         super(MyLSTMVAE, self).__init__()
         self.input_size = input_size
         self.hidden_size = hidden_size
@@ -28,36 +30,40 @@ class MyLSTMVAE(nn.Module):
         self.data_train = data_train
         self.data_test = data_test
         self.code_config = code_config
-        self.myvae = MyVAE(data_train, in_channels, latent_dim, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type, learning_rate, batch_size)
+        self.device = device
+        self.seq_len = seq_len
+        self.myvae = MyVAE(data_train, in_channels, batch_size, total_batch, latent_dim, hidden_dims, beta, gamma, max_capacity, Capacity_max_iter, loss_type, learning_rate, batch_size, seq_len, device=device)
 
     def train_vae(self):
         self.myvae.train(self.num_epochs)
-        self.loss_vae_train, self.input_train, self.reconst_train = self.myvae.test(self.data_train, load_model=False)
-        self.loss_vae_test, self.input_test, self.reconst_test = self.myvae.test(self.data_test, load_model=False)
-        return
+        loss_vae_train, input_train, reconst_train = self.myvae.test(self.data_train, load_model=False)
+        loss_vae_test, input_test, reconst_test = self.myvae.test(self.data_test, load_model=False)
+        return loss_vae_train, input_train, reconst_train, loss_vae_test, input_test, reconst_test
 
     def embedding(self):
         self.myvae.eval()
         lstm_data_train = {}
-        lstm_data_train['train_lstm'] = self.data_train
-        lstm_data_train['test_lstm'] = self.data_test
+        lstm_data_train['train_lstm'] = self.data_train.to(self.device)
+        lstm_data_train['test_lstm'] = self.data_test.to(self.device)
         lstm_data_train['n_train_lstm'] = self.data_train.shape[0]
         lstm_data_train['n_test_lstm'] = self.data_test.shape[0]
-        self.embedding_train, self.embedding_test = produce_embeddings(self.code_config, self.myvae, lstm_data_train)
-        self.gt_train, self.gt_test = produce_outputs(self.code_config, lstm_data_train)
-        return
+        embedding_train, embedding_test = produce_embeddings(self.code_config, self.myvae, lstm_data_train, self.device)
+        self.data_train_lstm, self.data_test_lstm = produce_predicts(self.code_config, embedding_train, embedding_test, self.device)
+        gt_train, gt_test = produce_outputs(self.code_config, lstm_data_train)
+        return gt_train.cpu().detach().numpy(), gt_test.cpu().detach().numpy()
 
     def train_lstm(self):
-        self.mylstm = MyLSTM(self.embedding_train, self.input_size, self.hidden_size, self.latent_size, num_layers=self.num_layers, learning_rate=self.learning_rate, batch_size=self.batch_size)
+        self.mylstm = MyLSTM(self.data_train_lstm, self.input_size, self.hidden_size, self.latent_size, self.code_config, num_layers=self.num_layers, learning_rate=self.learning_rate, batch_size=self.batch_size, device=self.device)
         self.mylstm.train(self.num_epochs)
-        self.loss_lstm_train, self.input_lstm_train, self.pred_lstm_train = self.mylstm.test(self.embedding_train, load_model=False)
-        self.loss_lstm_test, self.input_lstm_test, self.pred_lstm_test = self.mylstm.test(self.embedding_test, load_model=False)
-        return
+        loss_lstm_train, input_lstm_train, pred_lstm_train = self.mylstm.test(self.data_train_lstm, load_model=False)
+        loss_lstm_test, input_lstm_test, pred_lstm_test = self.mylstm.test(self.data_test_lstm, load_model=False)
+        return loss_lstm_train, input_lstm_train, pred_lstm_train, loss_lstm_test, input_lstm_test, pred_lstm_test
     
-    def anomaly_detect(self):
-        self.reconst_pred = self.myvae.decode(self.pred_lstm_test)
-        
-        return
+    def anomaly_detect(self, pred_lstm): #input the prediction of LSTM and ground truth ouput of decoder from here.
+        pred_lstm = pred_lstm.reshape(pred_lstm.shape[0], -1) #reshape to hidden state
+        pred = Variable(pt.tensor(pred_lstm)).to(self.device)
+        reconst = self.myvae.decode(pred)
+        return reconst.cpu().detach().numpy()
         
         
         
\ No newline at end of file
diff --git a/src/model/convnets.py b/src/model/convnets.py
deleted file mode 100644
index 7ec0fc1..0000000
--- a/src/model/convnets.py
+++ /dev/null
@@ -1,63 +0,0 @@
-import torch as pt
-import torch.nn as nn
-import torch.nn.functional as func
-from torch.autograd import Variable
-
-class Down1D(nn.Module):
-    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
-        super(Down1D, self).__init__()
-        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)
-        self.bn = nn.BatchNorm1d(out_channels)
-        self.relu = nn.ReLU()
-
-    def forward(self, x):
-        out = self.conv(x)
-        out = self.bn(out)
-        out = self.relu(out)
-        return out
-
-class Up1D(nn.Module):
-    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
-        super(Up1D, self).__init__()
-        self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding)
-        self.bn = nn.BatchNorm1d(out_channels)
-        self.relu = nn.ReLU()
-
-    def forward(self, x):
-        out = self.conv(x)
-        out = self.bn(out)
-        out = self.relu(out)
-        return out
-
-class PixelShuffle1D(nn.Module):
-    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, upscale_factor):
-        super(PixelShuffle1D, self).__init__()
-        self.conv = nn.Conv1d(in_channels, out_channels * upscale_factor, kernel_size, stride, padding)
-        self.bn = nn.BatchNorm1d(out_channels * upscale_factor)
-        self.relu = nn.ReLU()
-        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)
-
-    def forward(self, x):
-        out = self.conv(x)
-        out = self.bn(out)
-        out = self.relu(out)
-        out = self.pixel_shuffle(out)
-        return out
-
-class PixelUnshuffle1D(nn.Module):
-    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downscale_factor):
-        super(PixelUnshuffle1D, self).__init__()
-        self.conv = nn.Conv1d(in_channels, out_channels // downscale_factor, kernel_size, stride, padding)
-        self.bn = nn.BatchNorm1d(out_channels // downscale_factor)
-        self.relu = nn.ReLU()
-        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor)
-
-    def forward(self, x):
-        out = self.conv(x)
-        out = self.bn(out)
-        out = self.relu(out)
-        out = self.pixel_unshuffle(out)
-        return out
-
-
-
diff --git a/src/model/rnns.py b/src/model/rnns.py
deleted file mode 100644
index ba47fec..0000000
--- a/src/model/rnns.py
+++ /dev/null
@@ -1,20 +0,0 @@
-import torch as pt
-import torch.nn as nn
-import torch.nn.functional as func
-from torch.autograd import Variable
-
-class denselstm(nn.Module):
-    def __init__(self, input_size, hidden_size, latent_size, num_layers=1):
-        super(denselstm, self).__init__()
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-        self.latent_size = latent_size
-        self.num_layers = num_layers
-        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)
-        self.fc = nn.Linear(hidden_size, latent_size)
-
-    def forward(self, x):
-        out, _ = self.lstm(x)
-        out = self.fc(out)
-        return out
-
diff --git a/src/model/vae.py b/src/model/vae.py
deleted file mode 100644
index 53204e1..0000000
--- a/src/model/vae.py
+++ /dev/null
@@ -1,173 +0,0 @@
-import torch as pt
-from torch import nn
-from torch.nn import functional as F
-
-
-class BetaVAE(nn.Module):
-    num_iter = 0 # Global static variable to keep track of iterations
-
-    def __init__(self,
-                 in_channels: int,
-                 latent_dim: int,
-                 hidden_dims = None,
-                 beta: int = 4,
-                 gamma: float = 1000.,
-                 max_capacity: int = 25,
-                 Capacity_max_iter: int = 1e4,
-                 loss_type: str = 'B',
-                 **kwargs) -> None:
-        super(BetaVAE, self).__init__()
-
-        self.latent_dim = latent_dim
-        self.beta = beta
-        self.gamma = gamma
-        self.loss_type = loss_type
-        self.C_max = pt.Tensor([max_capacity])
-        self.C_stop_iter = Capacity_max_iter
-
-        modules = []
-        if hidden_dims is None:
-            hidden_dims = [32, 64, 128, 256, 512]
-
-        # Build Encoder
-        for h_dim in hidden_dims:
-            modules.append(
-                nn.Sequential(
-                    nn.Conv2d(in_channels, out_channels=h_dim, kernel_size=3, stride=2, padding=1),
-                    nn.BatchNorm2d(h_dim),
-                    nn.LeakyReLU())
-            )
-            in_channels = h_dim
-
-        self.encoder = nn.Sequential(*modules)
-        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)
-        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)
-
-
-        # Build Decoder
-        modules = []
-
-        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)
-
-        hidden_dims.reverse()
-
-        for i in range(len(hidden_dims) - 1):
-            modules.append(
-                nn.Sequential(
-                    nn.ConvTranspose2d(hidden_dims[i],
-                                       hidden_dims[i + 1],
-                                       kernel_size=3,
-                                       stride = 2,
-                                       padding=1,
-                                       output_padding=1),
-                    nn.BatchNorm2d(hidden_dims[i + 1]),
-                    nn.LeakyReLU())
-            )
-
-
-
-        self.decoder = nn.Sequential(*modules)
-
-        self.final_layer = nn.Sequential(
-                            nn.ConvTranspose2d(hidden_dims[-1],
-                                               hidden_dims[-1],
-                                               kernel_size=3,
-                                               stride=2,
-                                               padding=1,
-                                               output_padding=1),
-                            nn.BatchNorm2d(hidden_dims[-1]),
-                            nn.LeakyReLU(),
-                            nn.Conv2d(hidden_dims[-1], out_channels= 3,
-                                      kernel_size= 3, padding= 1),
-                            nn.Tanh())
-
-    def encode(self, input):
-        """
-        Encodes the input by passing through the encoder network
-        and returns the latent codes.
-        :param input: (Tensor) Input tensor to encoder [N x C x H x W]
-        :return: (Tensor) List of latent codes
-        """
-        result = self.encoder(input)
-        result = pt.flatten(result, start_dim=1)
-
-        # Split the result into mu and var components
-        # of the latent Gaussian distribution
-        mu = self.fc_mu(result)
-        log_var = self.fc_var(result)
-
-        return [mu, log_var]
-
-    def decode(self, z):
-        result = self.decoder_input(z)
-        result = result.view(-1, 512, 2, 2)
-        result = self.decoder(result)
-        result = self.final_layer(result)
-        return result
-
-    def reparameterize(self, mu, logvar):
-        """
-        Will a single z be enough ti compute the expectation
-        for the loss??
-        :param mu: (Tensor) Mean of the latent Gaussian
-        :param logvar: (Tensor) Standard deviation of the latent Gaussian
-        :return:
-        """
-        std = pt.exp(0.5 * logvar)
-        eps = pt.randn_like(std)
-        return eps * std + mu
-
-    def forward(self, input):
-        mu, log_var = self.encode(input)
-        z = self.reparameterize(mu, log_var)
-        return  [self.decode(z), input, mu, log_var]
-
-    def loss_function(self,
-                      *args,
-                      **kwargs) -> dict:
-        self.num_iter += 1
-        recons = args[0]
-        input = args[1]
-        mu = args[2]
-        log_var = args[3]
-        kld_weight = kwargs['M_N']  # Account for the minibatch samples from the dataset
-
-        recons_loss =F.mse_loss(recons, input)
-
-        kld_loss = pt.mean(-0.5 * pt.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)
-
-        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl
-            loss = recons_loss + self.beta * kld_weight * kld_loss
-        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf
-            self.C_max = self.C_max.to(input.device)
-            C = pt.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])
-            loss = recons_loss + self.gamma * kld_weight* (kld_loss - C).abs()
-        else:
-            raise ValueError('Undefined loss type.')
-
-        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}
-
-    def sample(self,
-               num_samples,
-               current_device,):
-        """
-        Samples from the latent space and return the corresponding
-        image space map.
-        :param num_samples: (Int) Number of samples
-        :param current_device: (Int) Device to run the model
-        :return: (Tensor)
-        """
-        z = pt.randn(num_samples,
-                        self.latent_dim)
-
-        z = z.to(current_device)
-
-        samples = self.decode(z)
-        return samples
-
-    def generate(self, x):
-        """
-        Given an input image x, returns the reconstructed image
-        :param x: (Tensor) [B x C x H x W]
-        :return: (Tensor) [B x C x H x W]
-        """
\ No newline at end of file
diff --git a/utils/loaders.py b/utils/loaders.py
index 73e1321..2a7d00c 100644
--- a/utils/loaders.py
+++ b/utils/loaders.py
@@ -4,78 +4,77 @@ import csv
 from torch.utils.data import Dataset
 from torch.autograd import Variable
 
-def produce_embeddings(config, model_vae, data):
-    embedding_lstm_train = np.zeros((data['n_train_lstm'], config['l_seq'], config['code_size']))
+def vae_training_data(batch_len, csv_file=None, in_channel=2):
+    trainingdata={}
+    if csv_file==None:
+        print('****Using Fake Training Data with Normal Distribution****')
+        trainingdata=pt.abs(pt.randn(2000, in_channel, batch_len, 1))
+        trainingdata=Variable(trainingdata/pt.max(trainingdata))
+    else:
+        train_in = []
+        train_out = []
+        with open(csv_file) as csvfile:
+            csv_reader = csv.reader(csvfile, delimiter=",")
+            header = next(csv_reader)
+            for row in csv_reader:
+                train_in += [float(row[1]),]
+        total_len = int((len(train_in)//batch_len)*batch_len)
+        trainingdata = Variable(pt.FloatTensor(train_in[:total_len]).reshape(-1, 1, batch_len, 1))
+    return trainingdata, trainingdata.size(0)
+
+def produce_embeddings(config, model_vae, data, device):
+    embedding_lstm_train = Variable(pt.zeros((data['n_train_lstm'], config['code_size'], config['hidden_len'], 1))).to(device)
     for i in range(data['n_train_lstm']):
-        embedding_lstm_train[i] = model_vae.encode(data['train_lstm'][i])
+        hidden_para = model_vae.b_vae.encode(pt.unsqueeze(data['train_lstm'][i],0))
+        embedding_lstm_train[i] = model_vae.b_vae.reparameterize(hidden_para[0], hidden_para[1]).view(-1, 256, model_vae.b_vae.hidden_len, 1)
 
-    embedding_lstm_test = np.zeros((data['n_test_lstm'], config['l_seq'], config['code_size']))
+    embedding_lstm_test = Variable(pt.zeros((data['n_test_lstm'], config['code_size'], config['hidden_len'], 1))).to(device)
     for i in range(data['n_test_lstm']):
-        embedding_lstm_test[i] = model_vae.encode(data['test_lstm'][i])
+        hidden_para = model_vae.b_vae.encode(pt.unsqueeze(data['test_lstm'][i],0))
+        embedding_lstm_test[i] = model_vae.b_vae.reparameterize(hidden_para[0], hidden_para[1]).view(-1, 256, model_vae.b_vae.hidden_len, 1)
     return embedding_lstm_train, embedding_lstm_test
 
-def produce_predicts(config, embedding_lstm_train, embedding_lstm_test):
+def produce_predicts(config, embedding_lstm_train, embedding_lstm_test, device):#get lstm training data from VAE latents
     n_train = embedding_lstm_train.shape[0]
     n_test = embedding_lstm_test.shape[0]
-    lstm_train_in = np.zeros((n_train-config['num_in'], config['l_seq']*config['num_in'], config['code_size']))
-    lstm_test_in = np.zeros((n_test-config['num_in'], config['l_seq']*config['num_in'], config['code_size']))
-    lstm_train_out = np.zeros((n_train-config['num_in'], config['l_seq'], config['code_size']))
-    lstm_test_out = np.zeros((n_test-config['num_in'], config['l_seq'], config['code_size']))
+    lstm_train_in = Variable(pt.zeros((n_train-config['num_in'], config['code_size'], config['hidden_len']*config['num_in'], 1))).to(device)
+    lstm_test_in = Variable(pt.zeros((n_test-config['num_in'], config['code_size'], config['hidden_len']*config['num_in'], 1))).to(device)
+    lstm_train_out = Variable(pt.zeros((n_train-config['num_in'], config['code_size'], config['hidden_len'], 1))).to(device)
+    lstm_test_out = Variable(pt.zeros((n_test-config['num_in'], config['code_size'], config['hidden_len'], 1))).to(device)
     for i in range(n_train):
         if i+config['num_in']<n_train:
             for n in range(config['num_in']):
-                lstm_train_in[i, config['l_seq']*n:config['l_seq']*(n+1)] = embedding_lstm_train[i+n]
+                lstm_train_in[i, :, config['hidden_len']*n:config['hidden_len']*(n+1)] = embedding_lstm_train[i+n]
             lstm_train_out[i] = embedding_lstm_train[i+config['num_in']]
     for i in range(n_test):
         if i+config['num_in']<n_test:
             for n in range(config['num_in']):
-                lstm_test_in[i, config['l_seq']*n:config['l_seq']*(n+1)] = embedding_lstm_test[i+n]
+                lstm_test_in[i, :, config['hidden_len']*n:config['hidden_len']*(n+1)] = embedding_lstm_test[i+n]
             lstm_test_out[i] = embedding_lstm_test[i+config['num_in']]
-    return lstm_train_in, lstm_test_in, lstm_train_out, lstm_test_out
+    data_train_lstm = {}
+    data_test_lstm = {}
+    data_train_lstm['input'] = lstm_train_in
+    data_train_lstm['output'] = lstm_train_out
+    data_test_lstm['input'] = lstm_test_in
+    data_test_lstm['output'] = lstm_test_out
+    return data_train_lstm, data_test_lstm
 
 def produce_outputs(config, data): #output of the decoder
-    batch_len = data['train_lstm'].shape[1]
-    channel_num = data['train_lstm'].shape[2]
-    eval_train_out = np.zeros((data['n_train_lstm']-config['num_in'], batch_len, channel_num))
-    eval_test_out = np.zeros((data['n_test_lstm']-config['num_in'], batch_len, channel_num))
-    eval_train_out = data['train_lstm'][config['num_in']:]
-    eval_test_out = data['test_lstm'][config['num_in']:]
+    eval_train_out = data['train_lstm'][config['num_in']:,:,:,:]
+    eval_test_out = data['test_lstm'][config['num_in']:,:,:,:]
     return eval_train_out, eval_test_out
 
-def vae_training_data(batch_len, csv_file=None):
-    trainingdata={}
-    if csv_file==None:
-        print('****Using Fake Training Data with Normal Distribution****')
-        trainingdata['input']=Variable(pt.randn(2e3, batch_len,1))
-        trainingdata['output'] = Variable(pt.randn(2e3, batch_len, 1))
-    else:
-        train_in = []
-        train_out = []
-        with open(csv_file) as csvfile:
-            csv_reader = csv.reader(csvfile, delimiter=",")
-            header = next(csv_reader)
-            for row in csv_reader:
-                train_in += [float(row[1]),]
-                train_out += [float(row[1]), ]
-        total_len = int((len(train_in)//batch_len)*batch_len)
-        trainingdata['input'] = Variable(pt.FloatTensor(train_in[:total_len]).reshape(-1, batch_len, 1))
-        trainingdata['output'] = Variable(pt.FloatTensor(train_out[:total_len]).reshape(-1, batch_len, 1))
-    return trainingdata
-
 class anomaly_loader(Dataset):
 
     def __init__(self, dataset):
         self.dataset = dataset
-        self.out_dim = len(dataset['output'].shape)
+        self.out_dim = len(dataset.shape)
 
     def __len__(self):
-        return int(self.dataset['input'].shape[0])
+        return int(self.dataset.shape[0])
 
     def __getitem__(self, idx):
-        item={}
-        item['input'] = self.dataset['input'][idx,:]
-        item['output'] = self.dataset['output'][idx,:]
-        return item
+        return self.dataset[idx,:]
 
 class embedding_loader(Dataset):
 
diff --git a/utils/myplot.py b/utils/myplot.py
index 2dd2177..2525179 100644
--- a/utils/myplot.py
+++ b/utils/myplot.py
@@ -3,4 +3,27 @@ import numpy as np
 import os
 import sys
 import time
+import wandb
 
+ROOT_DIR = '/home/jupyter-chuanhao/data/chuanhao_anomaly/vaelstm/'
+RES_DIR = ROOT_DIR + 'results'
+
+def plt_compare_series(series_list, title, xlabel=None, ylabel=None, label_list=None):
+    for i in range(series_list[0].shape[1]):
+        fig, ax = plt.subplots(1,1,figsize=(6,3))
+        for idx, s in enumerate(series_list):
+            if label_list==None:
+                ax.plot(s[:,i,:,:].reshape((s.shape[0]*s.shape[2],)))
+            else:
+                ax.plot(s[:,i,:,:].reshape((s.shape[0]*s.shape[2],)), label=label_list[idx])
+        if label_list!=None:
+            ax.legend()
+        wandb.log(
+                   {
+                    f"comparison_{title}_{i}": wandb.Image(fig), 
+                   }, commit=False
+            )
+        plt.savefig(RES_DIR+f'/{title}_{i}.jpg')
+        plt.close()
+    return
+    
\ No newline at end of file
