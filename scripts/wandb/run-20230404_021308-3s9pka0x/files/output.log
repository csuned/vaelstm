****Using Fake Training Data with Normal Distribution****
****Using Fake Training Data with Normal Distribution****
BetaVAE(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Linear(in_features=2304, out_features=2304, bias=True)
  (fc_var): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder_input): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (0): ConvTranspose2d(32, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv2d(32, 2, kernel_size=(3, 1), stride=(1, 1), padding=same)
    (4): Tanh()
  )
)
Epoch [1/30], Step [1/62], Loss: 293095872.0000
Epoch [1/30], Step [11/62], Loss: 144498080.0000
Epoch [1/30], Step [21/62], Loss: 34433268.0000
Epoch [1/30], Step [31/62], Loss: 12994439.0000
Epoch [1/30], Step [41/62], Loss: 17333320.0000
Epoch [1/30], Step [51/62], Loss: 28505080.0000
Epoch [1/30], Step [61/62], Loss: 18935940.0000
Epoch [2/30], Step [1/62], Loss: 11190551.0000
Epoch [2/30], Step [11/62], Loss: 17009550.0000
Epoch [2/30], Step [21/62], Loss: 10341191.0000
Epoch [2/30], Step [31/62], Loss: 6791237.0000
Epoch [2/30], Step [41/62], Loss: 63949968.0000
Epoch [2/30], Step [51/62], Loss: 4222153.0000
Epoch [2/30], Step [61/62], Loss: 4337231.5000
Epoch [3/30], Step [1/62], Loss: 9270689.0000
Epoch [3/30], Step [11/62], Loss: 8108927.0000
Epoch [3/30], Step [21/62], Loss: 3372331.0000
Epoch [3/30], Step [31/62], Loss: 24116130.0000
Epoch [3/30], Step [41/62], Loss: 5922054.0000
Epoch [3/30], Step [51/62], Loss: 1984235.0000
Epoch [3/30], Step [61/62], Loss: 1631130.6250
Epoch [4/30], Step [1/62], Loss: 3219082.0000
Epoch [4/30], Step [11/62], Loss: 1307236.5000
Epoch [4/30], Step [21/62], Loss: 7940079.5000
Epoch [4/30], Step [31/62], Loss: 8255007.0000
Epoch [4/30], Step [41/62], Loss: 869828.3125
Epoch [4/30], Step [51/62], Loss: 2619632.0000
Epoch [4/30], Step [61/62], Loss: 246269.7812
Epoch [5/30], Step [1/62], Loss: 717350.1875
Epoch [5/30], Step [11/62], Loss: 655791.6875
Epoch [5/30], Step [21/62], Loss: 6895114.0000
Epoch [5/30], Step [31/62], Loss: 6643630.0000
Epoch [5/30], Step [41/62], Loss: 3380385.5000
Epoch [5/30], Step [51/62], Loss: 826173.6250
Epoch [5/30], Step [61/62], Loss: 1072577.5000
Epoch [6/30], Step [1/62], Loss: 196087.5625
Epoch [6/30], Step [11/62], Loss: 49384.0117
Epoch [6/30], Step [21/62], Loss: 134595.9531
Epoch [6/30], Step [31/62], Loss: 984122.3750
Epoch [6/30], Step [41/62], Loss: 296888.5312
Epoch [6/30], Step [51/62], Loss: 12640177.0000
Epoch [6/30], Step [61/62], Loss: 1017089.8750
Epoch [7/30], Step [1/62], Loss: 2922606.5000
Epoch [7/30], Step [11/62], Loss: 12483833.0000
Epoch [7/30], Step [21/62], Loss: 3476789.0000
Epoch [7/30], Step [31/62], Loss: 26322.9141
Epoch [7/30], Step [41/62], Loss: 237653.5156
Epoch [7/30], Step [51/62], Loss: 190534.7188
Epoch [7/30], Step [61/62], Loss: 249386.5625
Epoch [8/30], Step [1/62], Loss: 3007973.7500
Epoch [8/30], Step [11/62], Loss: 1164240.1250
Epoch [8/30], Step [21/62], Loss: 75142.0391
Epoch [8/30], Step [31/62], Loss: 117320.5547
Epoch [8/30], Step [41/62], Loss: 443473.4688
Epoch [8/30], Step [51/62], Loss: 269606.5938
Epoch [8/30], Step [61/62], Loss: 219082.1250
Epoch [9/30], Step [1/62], Loss: 485387.2812
Epoch [9/30], Step [11/62], Loss: 603674.0000
Epoch [9/30], Step [21/62], Loss: 294269.0938
Epoch [9/30], Step [31/62], Loss: 207403.0781
Epoch [9/30], Step [41/62], Loss: 435728.3125
Epoch [9/30], Step [51/62], Loss: 560145.1250
Epoch [9/30], Step [61/62], Loss: 493784.8438
Epoch [10/30], Step [1/62], Loss: 152378.2188
Epoch [10/30], Step [11/62], Loss: 1030519.1250
Epoch [10/30], Step [21/62], Loss: 229971.3125
Epoch [10/30], Step [31/62], Loss: 88974.3750
Epoch [10/30], Step [41/62], Loss: 52927.7500
Epoch [10/30], Step [51/62], Loss: 170021.4375
Epoch [10/30], Step [61/62], Loss: 1441975.2500
Epoch [11/30], Step [1/62], Loss: 333062.8750
Epoch [11/30], Step [11/62], Loss: 12148.9902
Epoch [11/30], Step [21/62], Loss: 316172.3750
Epoch [11/30], Step [31/62], Loss: 2761032.0000
Epoch [11/30], Step [41/62], Loss: 137904.5469
Epoch [11/30], Step [51/62], Loss: 535909.4375
Epoch [11/30], Step [61/62], Loss: 488730.7812
Epoch [12/30], Step [1/62], Loss: 1149708.2500
Epoch [12/30], Step [11/62], Loss: 586922.5000
Epoch [12/30], Step [21/62], Loss: 1267968.0000
Epoch [12/30], Step [31/62], Loss: 487659.8125
Epoch [12/30], Step [41/62], Loss: 965783.0000
Epoch [12/30], Step [51/62], Loss: 271620.2812
Epoch [12/30], Step [61/62], Loss: 380425.5625
Epoch [13/30], Step [1/62], Loss: 113084.8125
Epoch [13/30], Step [11/62], Loss: 759147.3125
Epoch [13/30], Step [21/62], Loss: 697228.4375
Epoch [13/30], Step [31/62], Loss: 423125.2812
Epoch [13/30], Step [41/62], Loss: 126053.2266
Epoch [13/30], Step [51/62], Loss: 96071.7344
Epoch [13/30], Step [61/62], Loss: 28587.1172
Epoch [14/30], Step [1/62], Loss: 244754.0938
Epoch [14/30], Step [11/62], Loss: 366263.0312
Epoch [14/30], Step [21/62], Loss: 207444.9219
Epoch [14/30], Step [31/62], Loss: 971493.0000
Epoch [14/30], Step [41/62], Loss: 172629.8438
Epoch [14/30], Step [51/62], Loss: 302006.2500
Epoch [14/30], Step [61/62], Loss: 781569.9375
Epoch [15/30], Step [1/62], Loss: 450767.2812
Epoch [15/30], Step [11/62], Loss: 1817603.3750
Epoch [15/30], Step [21/62], Loss: 2377365.0000
Epoch [15/30], Step [31/62], Loss: 1816513.5000
Epoch [15/30], Step [41/62], Loss: 2946799.2500
Epoch [15/30], Step [51/62], Loss: 968608.0000
Epoch [15/30], Step [61/62], Loss: 332669.0312
Epoch [16/30], Step [1/62], Loss: 572021.0000
Epoch [16/30], Step [11/62], Loss: 669306.5000
Epoch [16/30], Step [21/62], Loss: 255207.0781
Epoch [16/30], Step [31/62], Loss: 118437.0703
Epoch [16/30], Step [41/62], Loss: 852117.7500
Epoch [16/30], Step [51/62], Loss: 12262.3594
Epoch [16/30], Step [61/62], Loss: 298347.9375
Epoch [17/30], Step [1/62], Loss: 554988.6250
Epoch [17/30], Step [11/62], Loss: 136577.8594
Epoch [17/30], Step [21/62], Loss: 143595.4688
Epoch [17/30], Step [31/62], Loss: 777026.6250
Epoch [17/30], Step [41/62], Loss: 477941.5000
Epoch [17/30], Step [51/62], Loss: 549971.3125
Epoch [17/30], Step [61/62], Loss: 640512.2500
Epoch [18/30], Step [1/62], Loss: 12362.9707
Epoch [18/30], Step [11/62], Loss: 668227.9375
Epoch [18/30], Step [21/62], Loss: 187177.6719
Epoch [18/30], Step [31/62], Loss: 6041.0635
Epoch [18/30], Step [41/62], Loss: 378690.9688
Epoch [18/30], Step [51/62], Loss: 244007.3594
Epoch [18/30], Step [61/62], Loss: 74981.7031
Epoch [19/30], Step [1/62], Loss: 669202.5625
Epoch [19/30], Step [11/62], Loss: 194796.3438
Epoch [19/30], Step [21/62], Loss: 129221.4531
Epoch [19/30], Step [31/62], Loss: 489107.1250
Epoch [19/30], Step [41/62], Loss: 476953.9688
Epoch [19/30], Step [51/62], Loss: 597247.8125
Epoch [19/30], Step [61/62], Loss: 510858.7812
Epoch [20/30], Step [1/62], Loss: 987111.9375
Epoch [20/30], Step [11/62], Loss: 106734.7656
Epoch [20/30], Step [21/62], Loss: 434297.0938
Epoch [20/30], Step [31/62], Loss: 676501.2500
Epoch [20/30], Step [41/62], Loss: 259864.5781
Epoch [20/30], Step [51/62], Loss: 94503.6562
Epoch [20/30], Step [61/62], Loss: 554243.0625
Epoch [21/30], Step [1/62], Loss: 585154.3125
Epoch [21/30], Step [11/62], Loss: 15265.2402
Epoch [21/30], Step [21/62], Loss: 279144.2812
Epoch [21/30], Step [31/62], Loss: 844740.1250
Epoch [21/30], Step [41/62], Loss: 356540.1875
Epoch [21/30], Step [51/62], Loss: 386719.2188
Epoch [21/30], Step [61/62], Loss: 821706.5625
Epoch [22/30], Step [1/62], Loss: 136704.4531
Epoch [22/30], Step [11/62], Loss: 305445.1875
Epoch [22/30], Step [21/62], Loss: 342363.8438
Epoch [22/30], Step [31/62], Loss: 14539.0166
Epoch [22/30], Step [41/62], Loss: 834107.6250
Epoch [22/30], Step [51/62], Loss: 157928.7188
Epoch [22/30], Step [61/62], Loss: 599255.3125
Epoch [23/30], Step [1/62], Loss: 491981.5000
Epoch [23/30], Step [11/62], Loss: 1243935.6250
Epoch [23/30], Step [21/62], Loss: 442768.8125
Epoch [23/30], Step [31/62], Loss: 1256472.8750
Epoch [23/30], Step [41/62], Loss: 1220777.0000
Epoch [23/30], Step [51/62], Loss: 490056.0312
Epoch [23/30], Step [61/62], Loss: 1578073.0000
Epoch [24/30], Step [1/62], Loss: 674959.8750
Epoch [24/30], Step [11/62], Loss: 21745.2188
Epoch [24/30], Step [21/62], Loss: 536801.8125
Epoch [24/30], Step [31/62], Loss: 483828.0625
Epoch [24/30], Step [41/62], Loss: 338225.5938
Epoch [24/30], Step [51/62], Loss: 152046.4531
Epoch [24/30], Step [61/62], Loss: 5104.3174
Epoch [25/30], Step [1/62], Loss: 561236.3750
Epoch [25/30], Step [11/62], Loss: 394436.1250
Epoch [25/30], Step [21/62], Loss: 454327.3438
Epoch [25/30], Step [31/62], Loss: 567325.8125
Epoch [25/30], Step [41/62], Loss: 146469.6094
Epoch [25/30], Step [51/62], Loss: 540643.6875
Epoch [25/30], Step [61/62], Loss: 591293.8125
Epoch [26/30], Step [1/62], Loss: 106345.9062
Epoch [26/30], Step [11/62], Loss: 688837.3125
Epoch [26/30], Step [21/62], Loss: 620786.8750
Epoch [26/30], Step [31/62], Loss: 103546.6328
Epoch [26/30], Step [41/62], Loss: 269962.0625
Epoch [26/30], Step [51/62], Loss: 136276.7344
Epoch [26/30], Step [61/62], Loss: 167105.2188
Epoch [27/30], Step [1/62], Loss: 1070894.7500
Epoch [27/30], Step [11/62], Loss: 281166.0625
Epoch [27/30], Step [21/62], Loss: 1109852.7500
Epoch [27/30], Step [31/62], Loss: 31310.5723
Epoch [27/30], Step [41/62], Loss: 142922.4219
Epoch [27/30], Step [51/62], Loss: 193534.3906
Epoch [27/30], Step [61/62], Loss: 1111052.0000
Epoch [28/30], Step [1/62], Loss: 141383.1875
Epoch [28/30], Step [11/62], Loss: 248495.5938
Epoch [28/30], Step [21/62], Loss: 495426.1875
Epoch [28/30], Step [31/62], Loss: 799952.7500
Epoch [28/30], Step [41/62], Loss: 407692.6562
Epoch [28/30], Step [51/62], Loss: 90503.7109
Epoch [28/30], Step [61/62], Loss: 305302.6250
Epoch [29/30], Step [1/62], Loss: 453889.5938
Epoch [29/30], Step [11/62], Loss: 1071546.1250
Epoch [29/30], Step [21/62], Loss: 213445.6719
Epoch [29/30], Step [31/62], Loss: 167768.0156
Epoch [29/30], Step [41/62], Loss: 400175.5625
Epoch [29/30], Step [51/62], Loss: 1120717.0000
Epoch [29/30], Step [61/62], Loss: 1012907.5000
Epoch [30/30], Step [1/62], Loss: 30335.9160
Epoch [30/30], Step [11/62], Loss: 497212.8750
Epoch [30/30], Step [21/62], Loss: 89728.3672
Epoch [30/30], Step [31/62], Loss: 927296.1875
Epoch [30/30], Step [41/62], Loss: 1719703.6250
Epoch [30/30], Step [51/62], Loss: 109033.1250
Epoch [30/30], Step [61/62], Loss: 557628.6250
denselstm(
  (lstm): LSTM(256, 256, batch_first=True)
  (fc): Linear(in_features=6912, out_features=2304, bias=True)
)
Epoch [1/30], Step [1/62], Loss: 0.8015
Epoch [1/30], Step [11/62], Loss: 0.7993
Epoch [1/30], Step [21/62], Loss: 0.8069
Epoch [1/30], Step [31/62], Loss: 0.8027
Epoch [1/30], Step [41/62], Loss: 0.8024
Epoch [1/30], Step [51/62], Loss: 0.8015
Epoch [1/30], Step [61/62], Loss: 0.8046
Epoch [2/30], Step [1/62], Loss: 0.6983
Epoch [2/30], Step [11/62], Loss: 0.6830
Epoch [2/30], Step [21/62], Loss: 0.6761
Epoch [2/30], Step [31/62], Loss: 0.6828
Epoch [2/30], Step [41/62], Loss: 0.6906
Epoch [2/30], Step [51/62], Loss: 0.6968
Epoch [2/30], Step [61/62], Loss: 0.7013
Epoch [3/30], Step [1/62], Loss: 0.5304
Epoch [3/30], Step [11/62], Loss: 0.5422
Epoch [3/30], Step [21/62], Loss: 0.5479
Epoch [3/30], Step [31/62], Loss: 0.5596
Epoch [3/30], Step [41/62], Loss: 0.5746
Epoch [3/30], Step [51/62], Loss: 0.5790
Epoch [3/30], Step [61/62], Loss: 0.5875
Epoch [4/30], Step [1/62], Loss: 0.4330
Epoch [4/30], Step [11/62], Loss: 0.4352
Epoch [4/30], Step [21/62], Loss: 0.4403
Epoch [4/30], Step [31/62], Loss: 0.4548
Epoch [4/30], Step [41/62], Loss: 0.4585
Epoch [4/30], Step [51/62], Loss: 0.4673
Epoch [4/30], Step [61/62], Loss: 0.4783
Epoch [5/30], Step [1/62], Loss: 0.3553
Epoch [5/30], Step [11/62], Loss: 0.3611
Epoch [5/30], Step [21/62], Loss: 0.3642
Epoch [5/30], Step [31/62], Loss: 0.3727
Epoch [5/30], Step [41/62], Loss: 0.3778
Epoch [5/30], Step [51/62], Loss: 0.3848
Epoch [5/30], Step [61/62], Loss: 0.4013
Epoch [6/30], Step [1/62], Loss: 0.2908
Epoch [6/30], Step [11/62], Loss: 0.3009
Epoch [6/30], Step [21/62], Loss: 0.3112
Epoch [6/30], Step [31/62], Loss: 0.3142
Epoch [6/30], Step [41/62], Loss: 0.3196
Epoch [6/30], Step [51/62], Loss: 0.3223
Epoch [6/30], Step [61/62], Loss: 0.3307
Epoch [7/30], Step [1/62], Loss: 0.2666
Epoch [7/30], Step [11/62], Loss: 0.2624
Epoch [7/30], Step [21/62], Loss: 0.2727
Epoch [7/30], Step [31/62], Loss: 0.2728
Epoch [7/30], Step [41/62], Loss: 0.2774
Epoch [7/30], Step [51/62], Loss: 0.2856
Epoch [7/30], Step [61/62], Loss: 0.2845
Epoch [8/30], Step [1/62], Loss: 0.2351
Epoch [8/30], Step [11/62], Loss: 0.2378
Epoch [8/30], Step [21/62], Loss: 0.2422
Epoch [8/30], Step [31/62], Loss: 0.2409
Epoch [8/30], Step [41/62], Loss: 0.2441
Epoch [8/30], Step [51/62], Loss: 0.2490
Epoch [8/30], Step [61/62], Loss: 0.2514
Epoch [9/30], Step [1/62], Loss: 0.2141
Epoch [9/30], Step [11/62], Loss: 0.2171
Epoch [9/30], Step [21/62], Loss: 0.2166
Epoch [9/30], Step [31/62], Loss: 0.2199
Epoch [9/30], Step [41/62], Loss: 0.2185
Epoch [9/30], Step [51/62], Loss: 0.2231
Epoch [9/30], Step [61/62], Loss: 0.2261
Epoch [10/30], Step [1/62], Loss: 0.2034
Epoch [10/30], Step [11/62], Loss: 0.2002
Epoch [10/30], Step [21/62], Loss: 0.1999
Epoch [10/30], Step [31/62], Loss: 0.1999
Epoch [10/30], Step [41/62], Loss: 0.2008
Epoch [10/30], Step [51/62], Loss: 0.2040
Epoch [10/30], Step [61/62], Loss: 0.2023
Epoch [11/30], Step [1/62], Loss: 0.1845
Epoch [11/30], Step [11/62], Loss: 0.1866
Epoch [11/30], Step [21/62], Loss: 0.1860
Epoch [11/30], Step [31/62], Loss: 0.1860
Epoch [11/30], Step [41/62], Loss: 0.1856
Epoch [11/30], Step [51/62], Loss: 0.1861
Epoch [11/30], Step [61/62], Loss: 0.1887
Epoch [12/30], Step [1/62], Loss: 0.1779
Epoch [12/30], Step [11/62], Loss: 0.1767
Epoch [12/30], Step [21/62], Loss: 0.1756
Epoch [12/30], Step [31/62], Loss: 0.1752
Epoch [12/30], Step [41/62], Loss: 0.1753
Epoch [12/30], Step [51/62], Loss: 0.1746
Epoch [12/30], Step [61/62], Loss: 0.1753
Epoch [13/30], Step [1/62], Loss: 0.1665
Epoch [13/30], Step [11/62], Loss: 0.1648
Epoch [13/30], Step [21/62], Loss: 0.1686
Epoch [13/30], Step [31/62], Loss: 0.1654
Epoch [13/30], Step [41/62], Loss: 0.1657
Epoch [13/30], Step [51/62], Loss: 0.1652
Epoch [13/30], Step [61/62], Loss: 0.1662
Epoch [14/30], Step [1/62], Loss: 0.1603
Epoch [14/30], Step [11/62], Loss: 0.1594
Epoch [14/30], Step [21/62], Loss: 0.1569
Epoch [14/30], Step [31/62], Loss: 0.1590
Epoch [14/30], Step [41/62], Loss: 0.1572
Epoch [14/30], Step [51/62], Loss: 0.1571
Epoch [14/30], Step [61/62], Loss: 0.1568
Epoch [15/30], Step [1/62], Loss: 0.1517
Epoch [15/30], Step [11/62], Loss: 0.1535
Epoch [15/30], Step [21/62], Loss: 0.1529
Epoch [15/30], Step [31/62], Loss: 0.1519
Epoch [15/30], Step [41/62], Loss: 0.1505
Epoch [15/30], Step [51/62], Loss: 0.1518
Epoch [15/30], Step [61/62], Loss: 0.1515
Epoch [16/30], Step [1/62], Loss: 0.1493
Epoch [16/30], Step [11/62], Loss: 0.1472
Epoch [16/30], Step [21/62], Loss: 0.1470
Epoch [16/30], Step [31/62], Loss: 0.1457
Epoch [16/30], Step [41/62], Loss: 0.1446
Epoch [16/30], Step [51/62], Loss: 0.1456
Epoch [16/30], Step [61/62], Loss: 0.1440
Epoch [17/30], Step [1/62], Loss: 0.1409
Epoch [17/30], Step [11/62], Loss: 0.1431
Epoch [17/30], Step [21/62], Loss: 0.1403
Epoch [17/30], Step [31/62], Loss: 0.1420
Epoch [17/30], Step [41/62], Loss: 0.1409
Epoch [17/30], Step [51/62], Loss: 0.1389
Epoch [17/30], Step [61/62], Loss: 0.1395
Epoch [18/30], Step [1/62], Loss: 0.1399
Epoch [18/30], Step [11/62], Loss: 0.1398
Epoch [18/30], Step [21/62], Loss: 0.1374
Epoch [18/30], Step [31/62], Loss: 0.1387
Epoch [18/30], Step [41/62], Loss: 0.1351
Epoch [18/30], Step [51/62], Loss: 0.1355
Epoch [18/30], Step [61/62], Loss: 0.1358
Epoch [19/30], Step [1/62], Loss: 0.1377
Epoch [19/30], Step [11/62], Loss: 0.1342
Epoch [19/30], Step [21/62], Loss: 0.1338
Epoch [19/30], Step [31/62], Loss: 0.1332
Epoch [19/30], Step [41/62], Loss: 0.1320
Epoch [19/30], Step [51/62], Loss: 0.1315
Epoch [19/30], Step [61/62], Loss: 0.1319
Epoch [20/30], Step [1/62], Loss: 0.1295
Epoch [20/30], Step [11/62], Loss: 0.1309
Epoch [20/30], Step [21/62], Loss: 0.1291
Epoch [20/30], Step [31/62], Loss: 0.1304
Epoch [20/30], Step [41/62], Loss: 0.1292
Epoch [20/30], Step [51/62], Loss: 0.1287
Epoch [20/30], Step [61/62], Loss: 0.1289
Epoch [21/30], Step [1/62], Loss: 0.1324
Epoch [21/30], Step [11/62], Loss: 0.1290
Epoch [21/30], Step [21/62], Loss: 0.1291
Epoch [21/30], Step [31/62], Loss: 0.1269
Epoch [21/30], Step [41/62], Loss: 0.1256
Epoch [21/30], Step [51/62], Loss: 0.1262
Epoch [21/30], Step [61/62], Loss: 0.1262
Epoch [22/30], Step [1/62], Loss: 0.1239
Epoch [22/30], Step [11/62], Loss: 0.1275
Epoch [22/30], Step [21/62], Loss: 0.1253
Epoch [22/30], Step [31/62], Loss: 0.1251
Epoch [22/30], Step [41/62], Loss: 0.1245
Epoch [22/30], Step [51/62], Loss: 0.1229
Epoch [22/30], Step [61/62], Loss: 0.1217
Epoch [23/30], Step [1/62], Loss: 0.1263
Epoch [23/30], Step [11/62], Loss: 0.1232
Epoch [23/30], Step [21/62], Loss: 0.1226
Epoch [23/30], Step [31/62], Loss: 0.1218
Epoch [23/30], Step [41/62], Loss: 0.1213
Epoch [23/30], Step [51/62], Loss: 0.1203
Epoch [23/30], Step [61/62], Loss: 0.1201
Epoch [24/30], Step [1/62], Loss: 0.1212
Epoch [24/30], Step [11/62], Loss: 0.1205
Epoch [24/30], Step [21/62], Loss: 0.1207
Epoch [24/30], Step [31/62], Loss: 0.1193
Epoch [24/30], Step [41/62], Loss: 0.1185
Epoch [24/30], Step [51/62], Loss: 0.1184
Epoch [24/30], Step [61/62], Loss: 0.1178
Epoch [25/30], Step [1/62], Loss: 0.1218
Epoch [25/30], Step [11/62], Loss: 0.1201
Epoch [25/30], Step [21/62], Loss: 0.1197
Epoch [25/30], Step [31/62], Loss: 0.1183
Epoch [25/30], Step [41/62], Loss: 0.1168
Epoch [25/30], Step [51/62], Loss: 0.1166
Epoch [25/30], Step [61/62], Loss: 0.1165
Epoch [26/30], Step [1/62], Loss: 0.1184
Epoch [26/30], Step [11/62], Loss: 0.1182
Epoch [26/30], Step [21/62], Loss: 0.1181
Epoch [26/30], Step [31/62], Loss: 0.1164
Epoch [26/30], Step [41/62], Loss: 0.1171
Epoch [26/30], Step [51/62], Loss: 0.1154
Epoch [26/30], Step [61/62], Loss: 0.1145
Epoch [27/30], Step [1/62], Loss: 0.1122
Epoch [27/30], Step [11/62], Loss: 0.1160
Epoch [27/30], Step [21/62], Loss: 0.1143
Epoch [27/30], Step [31/62], Loss: 0.1141
Epoch [27/30], Step [41/62], Loss: 0.1130
Epoch [27/30], Step [51/62], Loss: 0.1130
Epoch [27/30], Step [61/62], Loss: 0.1128
Epoch [28/30], Step [1/62], Loss: 0.1132
Epoch [28/30], Step [11/62], Loss: 0.1143
Epoch [28/30], Step [21/62], Loss: 0.1124
Epoch [28/30], Step [31/62], Loss: 0.1121
Epoch [28/30], Step [41/62], Loss: 0.1119
Epoch [28/30], Step [51/62], Loss: 0.1118
Epoch [28/30], Step [61/62], Loss: 0.1115
Epoch [29/30], Step [1/62], Loss: 0.1140
Epoch [29/30], Step [11/62], Loss: 0.1121
Epoch [29/30], Step [21/62], Loss: 0.1110
Epoch [29/30], Step [31/62], Loss: 0.1113
Epoch [29/30], Step [41/62], Loss: 0.1104
Epoch [29/30], Step [51/62], Loss: 0.1110
Epoch [29/30], Step [61/62], Loss: 0.1091
Epoch [30/30], Step [1/62], Loss: 0.1120
Epoch [30/30], Step [11/62], Loss: 0.1116
Epoch [30/30], Step [21/62], Loss: 0.1107
Epoch [30/30], Step [31/62], Loss: 0.1096
Epoch [30/30], Step [41/62], Loss: 0.1088
Epoch [30/30], Step [51/62], Loss: 0.1083
Epoch [30/30], Step [61/62], Loss: 0.1081
