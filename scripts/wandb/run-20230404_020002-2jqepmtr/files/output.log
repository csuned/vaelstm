****Using Fake Training Data with Normal Distribution****
****Using Fake Training Data with Normal Distribution****
BetaVAE(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (fc_mu): Linear(in_features=2304, out_features=2304, bias=True)
  (fc_var): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder_input): Linear(in_features=2304, out_features=2304, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01)
    )
  )
  (final_layer): Sequential(
    (0): ConvTranspose2d(32, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), output_padding=(1, 0))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv2d(32, 2, kernel_size=(3, 1), stride=(1, 1), padding=same)
    (4): Tanh()
  )
)
Epoch [1/30], Step [1/62], Loss: 295153440.0000
Epoch [1/30], Step [11/62], Loss: 130580048.0000
Epoch [1/30], Step [21/62], Loss: 24917016.0000
Epoch [1/30], Step [31/62], Loss: 11303736.0000
Epoch [1/30], Step [41/62], Loss: 11467286.0000
Epoch [1/30], Step [51/62], Loss: 20204636.0000
Epoch [1/30], Step [61/62], Loss: 8458506.0000
Epoch [2/30], Step [1/62], Loss: 6571133.0000
Epoch [2/30], Step [11/62], Loss: 2800083.2500
Epoch [2/30], Step [21/62], Loss: 12092762.0000
Epoch [2/30], Step [31/62], Loss: 4234359.0000
Epoch [2/30], Step [41/62], Loss: 6287004.0000
Epoch [2/30], Step [51/62], Loss: 24970182.0000
Epoch [2/30], Step [61/62], Loss: 12298073.0000
Epoch [3/30], Step [1/62], Loss: 10234394.0000
Epoch [3/30], Step [11/62], Loss: 2339790.7500
Epoch [3/30], Step [21/62], Loss: 1424618.8750
Epoch [3/30], Step [31/62], Loss: 886228.6250
Epoch [3/30], Step [41/62], Loss: 4999451.0000
Epoch [3/30], Step [51/62], Loss: 2813605.2500
Epoch [3/30], Step [61/62], Loss: 1223238.5000
Epoch [4/30], Step [1/62], Loss: 3889697.2500
Epoch [4/30], Step [11/62], Loss: 16030312.0000
Epoch [4/30], Step [21/62], Loss: 491254.9062
Epoch [4/30], Step [31/62], Loss: 272225.4688
Epoch [4/30], Step [41/62], Loss: 344979.5625
Epoch [4/30], Step [51/62], Loss: 1049846.8750
Epoch [4/30], Step [61/62], Loss: 45868.2344
Epoch [5/30], Step [1/62], Loss: 182160.0938
Epoch [5/30], Step [11/62], Loss: 2192472.2500
Epoch [5/30], Step [21/62], Loss: 1030563.5000
Epoch [5/30], Step [31/62], Loss: 1772720.3750
Epoch [5/30], Step [41/62], Loss: 182823.0781
Epoch [5/30], Step [51/62], Loss: 567328.0625
Epoch [5/30], Step [61/62], Loss: 355285.7500
Epoch [6/30], Step [1/62], Loss: 4073125.2500
Epoch [6/30], Step [11/62], Loss: 82591.3125
Epoch [6/30], Step [21/62], Loss: 146648.8438
Epoch [6/30], Step [31/62], Loss: 1159527.1250
Epoch [6/30], Step [41/62], Loss: 1804278.8750
Epoch [6/30], Step [51/62], Loss: 3493897.7500
Epoch [6/30], Step [61/62], Loss: 1314542.2500
Epoch [7/30], Step [1/62], Loss: 8119693.0000
Epoch [7/30], Step [11/62], Loss: 715635.0625
Epoch [7/30], Step [21/62], Loss: 117205.9922
Epoch [7/30], Step [31/62], Loss: 342317.3125
Epoch [7/30], Step [41/62], Loss: 1243214.8750
Epoch [7/30], Step [51/62], Loss: 307717.8438
Epoch [7/30], Step [61/62], Loss: 27222.1719
Epoch [8/30], Step [1/62], Loss: 68856.6094
Epoch [8/30], Step [11/62], Loss: 7058704.5000
Epoch [8/30], Step [21/62], Loss: 2530748.2500
Epoch [8/30], Step [31/62], Loss: 1579676.5000
Epoch [8/30], Step [41/62], Loss: 904988.0625
Epoch [8/30], Step [51/62], Loss: 6991746.5000
Epoch [8/30], Step [61/62], Loss: 946932.8125
Epoch [9/30], Step [1/62], Loss: 506655.1250
Epoch [9/30], Step [11/62], Loss: 9895.1016
Epoch [9/30], Step [21/62], Loss: 2141170.5000
Epoch [9/30], Step [31/62], Loss: 517914.2188
Epoch [9/30], Step [41/62], Loss: 1026724.5625
Epoch [9/30], Step [51/62], Loss: 1436203.8750
Epoch [9/30], Step [61/62], Loss: 3518947.5000
Epoch [10/30], Step [1/62], Loss: 1187736.1250
Epoch [10/30], Step [11/62], Loss: 411673.5625
Epoch [10/30], Step [21/62], Loss: 4413857.5000
Epoch [10/30], Step [31/62], Loss: 680395.7500
Epoch [10/30], Step [41/62], Loss: 721298.1875
Epoch [10/30], Step [51/62], Loss: 260763.5469
Epoch [10/30], Step [61/62], Loss: 2198182.5000
Epoch [11/30], Step [1/62], Loss: 814499.3125
Epoch [11/30], Step [11/62], Loss: 243194.5938
Epoch [11/30], Step [21/62], Loss: 843740.0000
Epoch [11/30], Step [31/62], Loss: 2995309.7500
Epoch [11/30], Step [41/62], Loss: 2960016.5000
Epoch [11/30], Step [51/62], Loss: 466828.1250
Epoch [11/30], Step [61/62], Loss: 474826.2500
Epoch [12/30], Step [1/62], Loss: 579338.8125
Epoch [12/30], Step [11/62], Loss: 451759.2500
Epoch [12/30], Step [21/62], Loss: 928939.0000
Epoch [12/30], Step [31/62], Loss: 797802.9375
Epoch [12/30], Step [41/62], Loss: 75797.3359
Epoch [12/30], Step [51/62], Loss: 547668.1875
Epoch [12/30], Step [61/62], Loss: 739885.1875
Epoch [13/30], Step [1/62], Loss: 664946.0625
Epoch [13/30], Step [11/62], Loss: 643575.8125
Epoch [13/30], Step [21/62], Loss: 634912.0000
Epoch [13/30], Step [31/62], Loss: 270222.9375
Epoch [13/30], Step [41/62], Loss: 790498.6250
Epoch [13/30], Step [51/62], Loss: 958648.4375
Epoch [13/30], Step [61/62], Loss: 798018.4375
Epoch [14/30], Step [1/62], Loss: 3790011.0000
Epoch [14/30], Step [11/62], Loss: 386908.0938
Epoch [14/30], Step [21/62], Loss: 877598.0625
Epoch [14/30], Step [31/62], Loss: 211349.2656
Epoch [14/30], Step [41/62], Loss: 741599.3125
Epoch [14/30], Step [51/62], Loss: 21346.8223
Epoch [14/30], Step [61/62], Loss: 525118.5000
Epoch [15/30], Step [1/62], Loss: 2992188.2500
Epoch [15/30], Step [11/62], Loss: 711191.1875
Epoch [15/30], Step [21/62], Loss: 1081585.3750
Epoch [15/30], Step [31/62], Loss: 1200189.2500
Epoch [15/30], Step [41/62], Loss: 1558891.7500
Epoch [15/30], Step [51/62], Loss: 2111880.0000
Epoch [15/30], Step [61/62], Loss: 1960048.0000
Epoch [16/30], Step [1/62], Loss: 955443.7500
Epoch [16/30], Step [11/62], Loss: 757684.2500
Epoch [16/30], Step [21/62], Loss: 947033.1875
Epoch [16/30], Step [31/62], Loss: 150231.3750
Epoch [16/30], Step [41/62], Loss: 692060.5000
Epoch [16/30], Step [51/62], Loss: 4831506.0000
Epoch [16/30], Step [61/62], Loss: 3496660.2500
Epoch [17/30], Step [1/62], Loss: 902505.7500
Epoch [17/30], Step [11/62], Loss: 51972.1680
Epoch [17/30], Step [21/62], Loss: 633873.8125
Epoch [17/30], Step [31/62], Loss: 749684.3125
Epoch [17/30], Step [41/62], Loss: 457372.2188
Epoch [17/30], Step [51/62], Loss: 273944.6562
Epoch [17/30], Step [61/62], Loss: 989321.9375
Epoch [18/30], Step [1/62], Loss: 588698.6250
Epoch [18/30], Step [11/62], Loss: 665651.0625
Epoch [18/30], Step [21/62], Loss: 1556113.7500
Epoch [18/30], Step [31/62], Loss: 699764.9375
Epoch [18/30], Step [41/62], Loss: 1066566.5000
Epoch [18/30], Step [51/62], Loss: 202409.2812
Epoch [18/30], Step [61/62], Loss: 354985.5000
Epoch [19/30], Step [1/62], Loss: 1045775.4375
Epoch [19/30], Step [11/62], Loss: 1079081.7500
Epoch [19/30], Step [21/62], Loss: 108045.1172
Epoch [19/30], Step [31/62], Loss: 36429.4219
Epoch [19/30], Step [41/62], Loss: 790359.5000
Epoch [19/30], Step [51/62], Loss: 589393.8750
Epoch [19/30], Step [61/62], Loss: 699754.9375
Epoch [20/30], Step [1/62], Loss: 271737.3750
Epoch [20/30], Step [11/62], Loss: 261815.5625
Epoch [20/30], Step [21/62], Loss: 194772.7344
Epoch [20/30], Step [31/62], Loss: 15104.0713
Epoch [20/30], Step [41/62], Loss: 289289.7500
Epoch [20/30], Step [51/62], Loss: 1558550.1250
Epoch [20/30], Step [61/62], Loss: 94545.6172
Epoch [21/30], Step [1/62], Loss: 183722.9844
Epoch [21/30], Step [11/62], Loss: 642678.2500
Epoch [21/30], Step [21/62], Loss: 92324.0312
Epoch [21/30], Step [31/62], Loss: 412348.7812
Epoch [21/30], Step [41/62], Loss: 774990.0625
Epoch [21/30], Step [51/62], Loss: 255529.4219
Epoch [21/30], Step [61/62], Loss: 92989.6953
Epoch [22/30], Step [1/62], Loss: 313640.6250
Epoch [22/30], Step [11/62], Loss: 877473.1250
Epoch [22/30], Step [21/62], Loss: 706789.0000
Epoch [22/30], Step [31/62], Loss: 781400.6875
Epoch [22/30], Step [41/62], Loss: 272557.2812
Epoch [22/30], Step [51/62], Loss: 965431.0000
Epoch [22/30], Step [61/62], Loss: 172819.8750
Epoch [23/30], Step [1/62], Loss: 43833.9883
Epoch [23/30], Step [11/62], Loss: 140788.3281
Epoch [23/30], Step [21/62], Loss: 478520.1875
Epoch [23/30], Step [31/62], Loss: 193773.2812
Epoch [23/30], Step [41/62], Loss: 145335.2188
Epoch [23/30], Step [51/62], Loss: 620352.5000
Epoch [23/30], Step [61/62], Loss: 318169.6250
Epoch [24/30], Step [1/62], Loss: 817117.9375
Epoch [24/30], Step [11/62], Loss: 337431.6875
Epoch [24/30], Step [21/62], Loss: 853322.5000
Epoch [24/30], Step [31/62], Loss: 628868.1250
Epoch [24/30], Step [41/62], Loss: 236378.9219
Epoch [24/30], Step [51/62], Loss: 901233.4375
Epoch [24/30], Step [61/62], Loss: 333731.9062
Epoch [25/30], Step [1/62], Loss: 579662.8125
Epoch [25/30], Step [11/62], Loss: 483216.3125
Epoch [25/30], Step [21/62], Loss: 613215.6875
Epoch [25/30], Step [31/62], Loss: 347148.6562
Epoch [25/30], Step [41/62], Loss: 177331.2188
Epoch [25/30], Step [51/62], Loss: 427915.5938
Epoch [25/30], Step [61/62], Loss: 563774.5625
Epoch [26/30], Step [1/62], Loss: 519888.6562
Epoch [26/30], Step [11/62], Loss: 211490.8906
Epoch [26/30], Step [21/62], Loss: 46762.7188
Epoch [26/30], Step [31/62], Loss: 815306.6875
Epoch [26/30], Step [41/62], Loss: 70195.2109
Epoch [26/30], Step [51/62], Loss: 307219.0625
Epoch [26/30], Step [61/62], Loss: 565021.0625
Epoch [27/30], Step [1/62], Loss: 260248.2031
Epoch [27/30], Step [11/62], Loss: 605042.4375
Epoch [27/30], Step [21/62], Loss: 1259475.7500
Epoch [27/30], Step [31/62], Loss: 527174.0000
Epoch [27/30], Step [41/62], Loss: 266825.1875
Epoch [27/30], Step [51/62], Loss: 53824.9180
Epoch [27/30], Step [61/62], Loss: 152446.2812
Epoch [28/30], Step [1/62], Loss: 1227449.0000
Epoch [28/30], Step [11/62], Loss: 994347.1250
Epoch [28/30], Step [21/62], Loss: 2413.2888
Epoch [28/30], Step [31/62], Loss: 91771.1406
Epoch [28/30], Step [41/62], Loss: 138395.7969
Epoch [28/30], Step [51/62], Loss: 591490.7500
Epoch [28/30], Step [61/62], Loss: 670668.1250
Epoch [29/30], Step [1/62], Loss: 1322907.5000
Epoch [29/30], Step [11/62], Loss: 170824.5469
Epoch [29/30], Step [21/62], Loss: 594988.8125
Epoch [29/30], Step [31/62], Loss: 348928.9688
Epoch [29/30], Step [41/62], Loss: 1068296.3750
Epoch [29/30], Step [51/62], Loss: 491154.6875
Epoch [29/30], Step [61/62], Loss: 81811.4453
Epoch [30/30], Step [1/62], Loss: 479939.9688
Epoch [30/30], Step [11/62], Loss: 54166.8086
Epoch [30/30], Step [21/62], Loss: 63550.4883
Epoch [30/30], Step [31/62], Loss: 13664.7383
Epoch [30/30], Step [41/62], Loss: 689291.9375
Epoch [30/30], Step [51/62], Loss: 1075698.8750
Epoch [30/30], Step [61/62], Loss: 561712.2500
denselstm(
  (lstm): LSTM(256, 256, batch_first=True)
  (fc): Linear(in_features=6912, out_features=2304, bias=True)
)
Epoch [1/30], Step [1/62], Loss: 0.7969
Epoch [1/30], Step [11/62], Loss: 0.8028
Epoch [1/30], Step [21/62], Loss: 0.8019
Epoch [1/30], Step [31/62], Loss: 0.7988
Epoch [1/30], Step [41/62], Loss: 0.8008
Epoch [1/30], Step [51/62], Loss: 0.8003
Epoch [1/30], Step [61/62], Loss: 0.8022
Epoch [2/30], Step [1/62], Loss: 0.6915
Epoch [2/30], Step [11/62], Loss: 0.6827
Epoch [2/30], Step [21/62], Loss: 0.6773
Epoch [2/30], Step [31/62], Loss: 0.6790
Epoch [2/30], Step [41/62], Loss: 0.6884
Epoch [2/30], Step [51/62], Loss: 0.6966
Epoch [2/30], Step [61/62], Loss: 0.7064
Epoch [3/30], Step [1/62], Loss: 0.5368
Epoch [3/30], Step [11/62], Loss: 0.5387
Epoch [3/30], Step [21/62], Loss: 0.5508
Epoch [3/30], Step [31/62], Loss: 0.5570
Epoch [3/30], Step [41/62], Loss: 0.5602
Epoch [3/30], Step [51/62], Loss: 0.5797
Epoch [3/30], Step [61/62], Loss: 0.5840
Epoch [4/30], Step [1/62], Loss: 0.4263
Epoch [4/30], Step [11/62], Loss: 0.4340
Epoch [4/30], Step [21/62], Loss: 0.4371
Epoch [4/30], Step [31/62], Loss: 0.4524
Epoch [4/30], Step [41/62], Loss: 0.4623
Epoch [4/30], Step [51/62], Loss: 0.4699
Epoch [4/30], Step [61/62], Loss: 0.4755
Epoch [5/30], Step [1/62], Loss: 0.3474
Epoch [5/30], Step [11/62], Loss: 0.3573
Epoch [5/30], Step [21/62], Loss: 0.3633
Epoch [5/30], Step [31/62], Loss: 0.3768
Epoch [5/30], Step [41/62], Loss: 0.3713
Epoch [5/30], Step [51/62], Loss: 0.3870
Epoch [5/30], Step [61/62], Loss: 0.3906
Epoch [6/30], Step [1/62], Loss: 0.2986
Epoch [6/30], Step [11/62], Loss: 0.3062
Epoch [6/30], Step [21/62], Loss: 0.3085
Epoch [6/30], Step [31/62], Loss: 0.3146
Epoch [6/30], Step [41/62], Loss: 0.3229
Epoch [6/30], Step [51/62], Loss: 0.3300
Epoch [6/30], Step [61/62], Loss: 0.3295
Epoch [7/30], Step [1/62], Loss: 0.2622
Epoch [7/30], Step [11/62], Loss: 0.2658
Epoch [7/30], Step [21/62], Loss: 0.2717
Epoch [7/30], Step [31/62], Loss: 0.2763
Epoch [7/30], Step [41/62], Loss: 0.2776
Epoch [7/30], Step [51/62], Loss: 0.2826
Epoch [7/30], Step [61/62], Loss: 0.2832
Epoch [8/30], Step [1/62], Loss: 0.2367
Epoch [8/30], Step [11/62], Loss: 0.2374
Epoch [8/30], Step [21/62], Loss: 0.2411
Epoch [8/30], Step [31/62], Loss: 0.2458
Epoch [8/30], Step [41/62], Loss: 0.2442
Epoch [8/30], Step [51/62], Loss: 0.2466
Epoch [8/30], Step [61/62], Loss: 0.2484
Epoch [9/30], Step [1/62], Loss: 0.2149
Epoch [9/30], Step [11/62], Loss: 0.2159
Epoch [9/30], Step [21/62], Loss: 0.2177
Epoch [9/30], Step [31/62], Loss: 0.2194
Epoch [9/30], Step [41/62], Loss: 0.2268
Epoch [9/30], Step [51/62], Loss: 0.2215
Epoch [9/30], Step [61/62], Loss: 0.2284
Epoch [10/30], Step [1/62], Loss: 0.2016
Epoch [10/30], Step [11/62], Loss: 0.2031
Epoch [10/30], Step [21/62], Loss: 0.2022
Epoch [10/30], Step [31/62], Loss: 0.2020
Epoch [10/30], Step [41/62], Loss: 0.2022
Epoch [10/30], Step [51/62], Loss: 0.2025
Epoch [10/30], Step [61/62], Loss: 0.2046
Epoch [11/30], Step [1/62], Loss: 0.1846
Epoch [11/30], Step [11/62], Loss: 0.1871
Epoch [11/30], Step [21/62], Loss: 0.1862
Epoch [11/30], Step [31/62], Loss: 0.1853
Epoch [11/30], Step [41/62], Loss: 0.1857
Epoch [11/30], Step [51/62], Loss: 0.1869
Epoch [11/30], Step [61/62], Loss: 0.1874
Epoch [12/30], Step [1/62], Loss: 0.1743
Epoch [12/30], Step [11/62], Loss: 0.1755
Epoch [12/30], Step [21/62], Loss: 0.1743
Epoch [12/30], Step [31/62], Loss: 0.1744
Epoch [12/30], Step [41/62], Loss: 0.1756
Epoch [12/30], Step [51/62], Loss: 0.1753
Epoch [12/30], Step [61/62], Loss: 0.1759
Epoch [13/30], Step [1/62], Loss: 0.1624
Epoch [13/30], Step [11/62], Loss: 0.1670
Epoch [13/30], Step [21/62], Loss: 0.1651
Epoch [13/30], Step [31/62], Loss: 0.1654
Epoch [13/30], Step [41/62], Loss: 0.1649
Epoch [13/30], Step [51/62], Loss: 0.1656
Epoch [13/30], Step [61/62], Loss: 0.1657
Epoch [14/30], Step [1/62], Loss: 0.1607
Epoch [14/30], Step [11/62], Loss: 0.1607
Epoch [14/30], Step [21/62], Loss: 0.1587
Epoch [14/30], Step [31/62], Loss: 0.1599
Epoch [14/30], Step [41/62], Loss: 0.1561
Epoch [14/30], Step [51/62], Loss: 0.1572
Epoch [14/30], Step [61/62], Loss: 0.1572
Epoch [15/30], Step [1/62], Loss: 0.1522
Epoch [15/30], Step [11/62], Loss: 0.1550
Epoch [15/30], Step [21/62], Loss: 0.1535
Epoch [15/30], Step [31/62], Loss: 0.1523
Epoch [15/30], Step [41/62], Loss: 0.1519
Epoch [15/30], Step [51/62], Loss: 0.1503
Epoch [15/30], Step [61/62], Loss: 0.1495
Epoch [16/30], Step [1/62], Loss: 0.1511
Epoch [16/30], Step [11/62], Loss: 0.1481
Epoch [16/30], Step [21/62], Loss: 0.1471
Epoch [16/30], Step [31/62], Loss: 0.1460
Epoch [16/30], Step [41/62], Loss: 0.1447
Epoch [16/30], Step [51/62], Loss: 0.1456
Epoch [16/30], Step [61/62], Loss: 0.1450
Epoch [17/30], Step [1/62], Loss: 0.1440
Epoch [17/30], Step [11/62], Loss: 0.1422
Epoch [17/30], Step [21/62], Loss: 0.1422
Epoch [17/30], Step [31/62], Loss: 0.1410
Epoch [17/30], Step [41/62], Loss: 0.1417
Epoch [17/30], Step [51/62], Loss: 0.1402
Epoch [17/30], Step [61/62], Loss: 0.1407
Epoch [18/30], Step [1/62], Loss: 0.1392
Epoch [18/30], Step [11/62], Loss: 0.1376
Epoch [18/30], Step [21/62], Loss: 0.1374
Epoch [18/30], Step [31/62], Loss: 0.1383
Epoch [18/30], Step [41/62], Loss: 0.1371
Epoch [18/30], Step [51/62], Loss: 0.1361
Epoch [18/30], Step [61/62], Loss: 0.1353
Epoch [19/30], Step [1/62], Loss: 0.1371
Epoch [19/30], Step [11/62], Loss: 0.1350
Epoch [19/30], Step [21/62], Loss: 0.1346
Epoch [19/30], Step [31/62], Loss: 0.1335
Epoch [19/30], Step [41/62], Loss: 0.1323
Epoch [19/30], Step [51/62], Loss: 0.1309
Epoch [19/30], Step [61/62], Loss: 0.1297
Epoch [20/30], Step [1/62], Loss: 0.1315
Epoch [20/30], Step [11/62], Loss: 0.1312
Epoch [20/30], Step [21/62], Loss: 0.1306
Epoch [20/30], Step [31/62], Loss: 0.1298
Epoch [20/30], Step [41/62], Loss: 0.1297
Epoch [20/30], Step [51/62], Loss: 0.1285
Epoch [20/30], Step [61/62], Loss: 0.1292
Epoch [21/30], Step [1/62], Loss: 0.1303
Epoch [21/30], Step [11/62], Loss: 0.1287
Epoch [21/30], Step [21/62], Loss: 0.1271
Epoch [21/30], Step [31/62], Loss: 0.1288
Epoch [21/30], Step [41/62], Loss: 0.1269
Epoch [21/30], Step [51/62], Loss: 0.1260
Epoch [21/30], Step [61/62], Loss: 0.1266
Epoch [22/30], Step [1/62], Loss: 0.1249
Epoch [22/30], Step [11/62], Loss: 0.1268
Epoch [22/30], Step [21/62], Loss: 0.1245
Epoch [22/30], Step [31/62], Loss: 0.1243
Epoch [22/30], Step [41/62], Loss: 0.1239
Epoch [22/30], Step [51/62], Loss: 0.1225
Epoch [22/30], Step [61/62], Loss: 0.1230
Epoch [23/30], Step [1/62], Loss: 0.1237
Epoch [23/30], Step [11/62], Loss: 0.1228
Epoch [23/30], Step [21/62], Loss: 0.1230
Epoch [23/30], Step [31/62], Loss: 0.1225
Epoch [23/30], Step [41/62], Loss: 0.1219
Epoch [23/30], Step [51/62], Loss: 0.1217
Epoch [23/30], Step [61/62], Loss: 0.1217
Epoch [24/30], Step [1/62], Loss: 0.1208
Epoch [24/30], Step [11/62], Loss: 0.1217
Epoch [24/30], Step [21/62], Loss: 0.1203
Epoch [24/30], Step [31/62], Loss: 0.1204
Epoch [24/30], Step [41/62], Loss: 0.1182
Epoch [24/30], Step [51/62], Loss: 0.1194
Epoch [24/30], Step [61/62], Loss: 0.1175
Epoch [25/30], Step [1/62], Loss: 0.1227
Epoch [25/30], Step [11/62], Loss: 0.1196
Epoch [25/30], Step [21/62], Loss: 0.1194
Epoch [25/30], Step [31/62], Loss: 0.1176
Epoch [25/30], Step [41/62], Loss: 0.1165
Epoch [25/30], Step [51/62], Loss: 0.1171
Epoch [25/30], Step [61/62], Loss: 0.1168
Epoch [26/30], Step [1/62], Loss: 0.1170
Epoch [26/30], Step [11/62], Loss: 0.1178
Epoch [26/30], Step [21/62], Loss: 0.1168
Epoch [26/30], Step [31/62], Loss: 0.1156
Epoch [26/30], Step [41/62], Loss: 0.1159
Epoch [26/30], Step [51/62], Loss: 0.1149
Epoch [26/30], Step [61/62], Loss: 0.1133
Epoch [27/30], Step [1/62], Loss: 0.1176
Epoch [27/30], Step [11/62], Loss: 0.1152
Epoch [27/30], Step [21/62], Loss: 0.1140
Epoch [27/30], Step [31/62], Loss: 0.1135
Epoch [27/30], Step [41/62], Loss: 0.1132
Epoch [27/30], Step [51/62], Loss: 0.1136
Epoch [27/30], Step [61/62], Loss: 0.1135
Epoch [28/30], Step [1/62], Loss: 0.1146
Epoch [28/30], Step [11/62], Loss: 0.1136
Epoch [28/30], Step [21/62], Loss: 0.1137
Epoch [28/30], Step [31/62], Loss: 0.1137
Epoch [28/30], Step [41/62], Loss: 0.1119
Epoch [28/30], Step [51/62], Loss: 0.1119
Epoch [28/30], Step [61/62], Loss: 0.1105
Epoch [29/30], Step [1/62], Loss: 0.1136
Epoch [29/30], Step [11/62], Loss: 0.1122
Epoch [29/30], Step [21/62], Loss: 0.1112
Epoch [29/30], Step [31/62], Loss: 0.1102
Epoch [29/30], Step [41/62], Loss: 0.1089
Epoch [29/30], Step [51/62], Loss: 0.1107
Epoch [29/30], Step [61/62], Loss: 0.1100
Epoch [30/30], Step [1/62], Loss: 0.1098
Epoch [30/30], Step [11/62], Loss: 0.1102
Epoch [30/30], Step [21/62], Loss: 0.1100
Epoch [30/30], Step [31/62], Loss: 0.1094
Epoch [30/30], Step [41/62], Loss: 0.1090
Epoch [30/30], Step [51/62], Loss: 0.1082
Epoch [30/30], Step [61/62], Loss: 0.1073
